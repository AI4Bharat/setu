{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can refer to the [commands](./commands.md) file for the list of commands utilized in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /home/shanks/hadoop/spark-3.5.1/logs/spark-shanks-org.apache.spark.deploy.master.Master-1-YDEARYZEN.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /home/shanks/hadoop/spark-3.5.1/logs/spark-shanks-org.apache.spark.deploy.worker.Worker-1-YDEARYZEN.out\n"
     ]
    }
   ],
   "source": [
    "!$SPARK_HOME/sbin/start-master.sh\n",
    "!$SPARK_HOME/sbin/start-worker.sh spark://YDEARYZEN.:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shanks\n"
     ]
    }
   ],
   "source": [
    "!export USER=\"shanks\"\n",
    "!echo $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "import pathlib\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EnathuKathaikalinKathaikal',\n",
       " 'AnaithulagaManithanaiNokki',\n",
       " 'BharathidasanThalattuPaadalgal',\n",
       " 'History-Church-Of-Mary-Magdalene-Kottapalayam',\n",
       " 'IlaignarkalukkuPeriyarVaralaru',\n",
       " 'GIRLSCANBECOMEWHATTHEYWANTTAMIL',\n",
       " 'AMAZINGACTIVITIESINTAMIL',\n",
       " 'AgamumPuramum',\n",
       " 'DESMONDTUTUTAMIL_201812',\n",
       " 'ITCOULDALWAYSBEWORSETAMIL',\n",
       " 'AMAZINGBONETAMIL',\n",
       " 'AriviyalNokkilKalamumKadigaramum',\n",
       " 'EVOLUTIONTAMIL',\n",
       " 'ALBERTEINSTEINTAMIL',\n",
       " 'HOUSESFROMTHESEATAMIL']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(f\"/home/{os.environ.get('USER')}/setu/examples/sample_data/tamil_ocr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/29 10:45:06 WARN Utils: Your hostname, YDEARYZEN resolves to a loopback address: 127.0.1.1; using 172.31.108.238 instead (on interface eth0)\n",
      "24/03/29 10:45:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/29 10:45:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "shanks\n",
      "------------------------------------------------ Setting Environment Variables --------------------------------------------------\n",
      "------------------------------------------------ END --------------------------------------------------\n",
      "------------------------------------------------ Environment Variables --------------------------------------------------\n",
      "PATH: /home/shanks/miniconda3/envs/setu_env/bin:/home/shanks/.vscode-server/bin/863d2581ecda6849923a2118d93a088b0745d9d6/bin/remote-cli:/home/shanks/hadoop/spark-3.5.1/bin:/home/shanks/miniconda3/envs/setu_env/bin:/home/shanks/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files (x86)/VMware/VMware Player/bin:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/dotnet:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files/Go/bin:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/RabbitMQ Server/rabbitmq_server-3.12.13/sbin:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311/Scripts:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311:/mnt/c/Users/Shanks/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Shanks/AppData/Roaming/npm:/mnt/c/Users/Shanks/AppData/Local/Google/Cloud SDK/google-cloud-sdk/bin:/mnt/c/Users/Shanks/miniconda3/Scripts:/mnt/c/Users/Shanks/miniconda3:/mnt/c/Users/Shanks/miniconda3/Library/bin/conda.bat:/snap/bin:/home/hadoop/hadoop-3.4.0/sbin:/home/hadoop/hadoop-3.4.0/bin\n",
      "XDG_DATA_DIRS: /usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "CONDA_DEFAULT_ENV: setu_env\n",
      "GIT_PAGER: cat\n",
      "CLICOLOR_FORCE: 1\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "SPARK_ENV_LOADED: 1\n",
      "SPARK_CLASSPATH: :/path/to/your/hadoop-lzo/java/libs\n",
      "CONDA_PYTHON_EXE: /home/shanks/miniconda3/bin/python\n",
      "WSL2_GUI_APPS_ENABLED: 1\n",
      "DBUS_SESSION_BUS_ADDRESS: unix:path=/run/user/1000/bus\n",
      "FILTER_DATA_ROOT: /home/shanks/setu/setu/data\n",
      "CONDA_PREFIX: /home/shanks/miniconda3/envs/setu_env\n",
      "CONDA_PREFIX_1: /home/shanks/miniconda3\n",
      "LOGNAME: shanks\n",
      "SETU_TMP_DIR: /home/shanks/tmp/\n",
      "PWD: /home/shanks/setu/examples\n",
      "VSCODE_NLS_CONFIG: {\"locale\":\"en\",\"osLocale\":\"en\",\"availableLanguages\":{}}\n",
      "SPARK_AUTH_SOCKET_TIMEOUT: 15\n",
      "HADOOP_INSTALL: /home/hadoop/hadoop-3.4.0\n",
      "PYTHONPATH: /home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip:/home/shanks/hadoop/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip:/home/shanks/hadoop/spark-3.5.1/jars/spark-core_2.12-3.5.1.jar\n",
      "LESSOPEN: | /usr/bin/lesspipe %s\n",
      "SHELL: /bin/bash\n",
      "WSL_INTEROP: /run/WSL/403_interop\n",
      "CONDA_ROOT: /home/shanks/miniconda3\n",
      "PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "HOSTTYPE: x86_64\n",
      "SPARK_BUFFER_SIZE: 65536\n",
      "HADOOP_YARN_HOME: /home/hadoop/hadoop-3.4.0\n",
      "VSCODE_WSL_EXT_LOCATION: /mnt/c/Users/Shanks/.vscode/extensions/ms-vscode-remote.remote-wsl-0.86.0\n",
      "SPARK_CONF_DIR: /home/shanks/hadoop/spark-3.5.1/conf\n",
      "NAME: YDEARYZEN\n",
      "HADOOP_HOME: /home/hadoop/hadoop-3.4.0\n",
      "CONDA_PROMPT_MODIFIER: (setu_env) \n",
      "LS_COLORS: rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\n",
      "SHLVL: 1\n",
      "GSETTINGS_SCHEMA_DIR_CONDA_BACKUP: \n",
      "SETU_DIR: /home/shanks/setu\n",
      "VSCODE_HANDLES_SIGPIPE: true\n",
      "LESSCLOSE: /usr/bin/lesspipe %s %s\n",
      "FORCE_COLOR: 1\n",
      "VSCODE_L10N_BUNDLE_LOCATION: \n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "CONDA_EXE: /home/shanks/miniconda3/bin/conda\n",
      "TERM: xterm-color\n",
      "SPARK_LIBRARY_PATH: :/path/to/your/hadoop-lzo/libs/native\n",
      "LANG: C.UTF-8\n",
      "SPARK_SCALA_VERSION: 2.12\n",
      "DISPLAY: :0\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "SPARK_HOME: /home/shanks/hadoop/spark-3.5.1\n",
      "_CE_M: \n",
      "WAYLAND_DISPLAY: wayland-0\n",
      "VSCODE_IPC_HOOK_CLI: /run/user/1000/vscode-ipc-ed3e9043-c9db-4767-a53f-5f994b854bba.sock\n",
      "VSCODE_AMD_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "HADOOP_HDFS_HOME: /home/hadoop/hadoop-3.4.0\n",
      "HADOOP_MAPRED_HOME: /home/hadoop/hadoop-3.4.0\n",
      "HADOOP_COMMON_HOME: /home/hadoop/hadoop-3.4.0\n",
      "CONDA_SHLVL: 2\n",
      "PYTHONHASHSEED: 0\n",
      "VSCODE_CWD: /mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code\n",
      "WSL_DISTRO_NAME: Ubuntu\n",
      "PYTHONIOENCODING: utf-8\n",
      "HADOOP_OPTS: ”-Djava.library.path=/home/hadoop/hadoop-3.4.0/lib/native”\n",
      "PYSPARK_GATEWAY_SECRET: 13ddd3693de8ec2169fd19247b011dca023dd37220cbb22babbbed7218550ed9\n",
      "USER: shanks\n",
      "CLICOLOR: 1\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "PULSE_SERVER: unix:/mnt/wslg/PulseServer\n",
      "PYSPARK_GATEWAY_PORT: 37863\n",
      "_CE_CONDA: \n",
      "PYTHONUNBUFFERED: YES\n",
      "WSLENV: VSCODE_WSL_EXT_LOCATION/up\n",
      "HADOOP_COMMON_LIB_NATIVE_DIR: /home/hadoop/hadoop-3.4.0/lib/native\n",
      "GSETTINGS_SCHEMA_DIR: /home/shanks/miniconda3/envs/setu_env/share/glib-2.0/schemas\n",
      "XDG_RUNTIME_DIR: /run/user/1000/\n",
      "HOME: /home/shanks\n",
      "------------------------------------------------ END --------------------------------------------------\n",
      "------------------------------------------------ Directory Structure --------------------------------------------------\n",
      "commands.md\n",
      "crawl_demo.ipynb\n",
      "ocr_demo.ipynb\n",
      "output\n",
      "sample_data\n",
      "ls: cannot access 'setu.zip': No such file or directory\n",
      "------------------------------------------------ End --------------------------------------------------\n",
      "Command used to run this script:  /home/shanks/setu/setu/run.py --config /home/shanks/setu/configs/ocr/spark_tamil_config.json --mode ocr --run_local True FormatConversionStage --fc_json_glob /home/shanks/setu/examples/sample_data/tamil/*/*/*.json --fc_samples_per_partition 1500 --fc_run_mode stage --fc_output_path /home/shanks/setu/examples/output/fc_output\n",
      "Entered Commandline Arguments:  Namespace(config='/home/shanks/setu/configs/ocr/spark_tamil_config.json', mode='ocr', run_local=True, stage='FormatConversionStage', fc_json_glob='/home/shanks/setu/examples/sample_data/tamil/*/*/*.json', fc_samples_per_partition=1500, fc_run_mode='stage', fc_output_path='/home/shanks/setu/examples/output/fc_output')\n",
      "PySpark Installed. Going forward with spark execution for JSON2ParquetStage\n",
      "PySpark Installed. Going forward with spark execution for ExtractTextStage\n",
      "PySpark Installed. Going forward with spark execution for DocCleanStage\n",
      "PySpark Installed. Going forward with spark execution for LIDStage\n",
      "PySpark Installed. Going forward with spark execution for AnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for FlaggingAndFilteringStage\n",
      "PySpark Installed. Going forward with spark execution for DocumentRemovalStage\n",
      "PySpark Installed. Going forward with spark execution for FormatConversionStage\n",
      "24/03/29 10:45:10 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/03/29 10:45:10 INFO SparkContext: OS info Linux, 5.15.133.1-microsoft-standard-WSL2, amd64\n",
      "24/03/29 10:45:10 INFO SparkContext: Java version 11.0.22\n",
      "24/03/29 10:45:10 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/03/29 10:45:11 INFO ResourceUtils: ==============================================================\n",
      "24/03/29 10:45:11 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/03/29 10:45:11 INFO ResourceUtils: ==============================================================\n",
      "24/03/29 10:45:11 INFO SparkContext: Submitted application: Sangraha Anaylsis\n",
      "24/03/29 10:45:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/03/29 10:45:11 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "24/03/29 10:45:11 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/03/29 10:45:11 INFO SecurityManager: Changing view acls to: shanks\n",
      "24/03/29 10:45:11 INFO SecurityManager: Changing modify acls to: shanks\n",
      "24/03/29 10:45:11 INFO SecurityManager: Changing view acls groups to: \n",
      "24/03/29 10:45:11 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/03/29 10:45:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: shanks; groups with view permissions: EMPTY; users with modify permissions: shanks; groups with modify permissions: EMPTY\n",
      "24/03/29 10:45:11 INFO Utils: Successfully started service 'sparkDriver' on port 37133.\n",
      "24/03/29 10:45:11 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/03/29 10:45:11 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/03/29 10:45:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/03/29 10:45:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/03/29 10:45:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/03/29 10:45:11 INFO DiskBlockManager: Created local directory at /home/shanks/tmp/blockmgr-64891429-c7d3-45a0-8bf5-370f5f5f17c3\n",
      "24/03/29 10:45:11 INFO MemoryStore: MemoryStore started with capacity 3.4 GiB\n",
      "24/03/29 10:45:11 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/03/29 10:45:11 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/03/29 10:45:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/29 10:45:11 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "24/03/29 10:45:11 INFO SparkContext: Added archive file:///home/shanks/setu/dataproc/envs/setu.zip at spark://localhost:37133/files/setu.zip with timestamp 1711689310987\n",
      "24/03/29 10:45:11 INFO Utils: Copying /home/shanks/setu/dataproc/envs/setu.zip to /home/shanks/tmp/spark-def91191-96f1-43f8-bbe8-630c36984b07/setu.zip\n",
      "24/03/29 10:45:17 INFO SparkContext: Unpacking an archive file:///home/shanks/setu/dataproc/envs/setu.zip from /home/shanks/tmp/spark-def91191-96f1-43f8-bbe8-630c36984b07/setu.zip to /home/shanks/tmp/spark-3cafdb11-916e-4092-aa99-1b1c55818a8a/userFiles-91371ca1-92ff-4390-b82e-0e2d35cae125/setu.zip\n",
      "24/03/29 10:45:39 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://YDEARYZEN.:7077...\n",
      "24/03/29 10:45:39 INFO TransportClientFactory: Successfully created connection to YDEARYZEN./127.0.1.1:7077 after 37 ms (0 ms spent in bootstraps)\n",
      "24/03/29 10:45:40 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240329104540-0010\n",
      "24/03/29 10:45:40 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240329104540-0010/0 on worker-20240329090107-172.31.108.238-39477 (172.31.108.238:39477) with 2 core(s)\n",
      "24/03/29 10:45:40 INFO StandaloneSchedulerBackend: Granted executor ID app-20240329104540-0010/0 on hostPort 172.31.108.238:39477 with 2 core(s), 3.0 GiB RAM\n",
      "24/03/29 10:45:40 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240329104540-0010/1 on worker-20240329090107-172.31.108.238-39477 (172.31.108.238:39477) with 2 core(s)\n",
      "24/03/29 10:45:40 INFO StandaloneSchedulerBackend: Granted executor ID app-20240329104540-0010/1 on hostPort 172.31.108.238:39477 with 2 core(s), 3.0 GiB RAM\n",
      "24/03/29 10:45:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36475.\n",
      "24/03/29 10:45:40 INFO NettyBlockTransferService: Server created on localhost:36475\n",
      "24/03/29 10:45:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/03/29 10:45:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 36475, None)\n",
      "24/03/29 10:45:40 INFO BlockManagerMasterEndpoint: Registering block manager localhost:36475 with 3.4 GiB RAM, BlockManagerId(driver, localhost, 36475, None)\n",
      "24/03/29 10:45:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 36475, None)\n",
      "24/03/29 10:45:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 36475, None)\n",
      "24/03/29 10:45:40 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240329104540-0010/0 is now RUNNING\n",
      "24/03/29 10:45:40 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240329104540-0010/1 is now RUNNING\n",
      "24/03/29 10:45:40 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "Setu: running `FormatConversionStage` stage/component\n",
      "24/03/29 10:45:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/03/29 10:45:41 INFO SharedState: Warehouse path is 'file:/home/shanks/setu/examples/spark-warehouse'.\n",
      "24/03/29 10:45:42 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 87 paths. The first several paths are: file:/home/shanks/setu/examples/sample_data/tamil/ALBERTEINSTEINTAMIL/ALBERT_EINSTEIN_-_TAMIL/output-1-to-20.json, file:/home/shanks/setu/examples/sample_data/tamil/ALBERTEINSTEINTAMIL/ALBERT_EINSTEIN_-_TAMIL/output-21-to-27.json, file:/home/shanks/setu/examples/sample_data/tamil/AMAZINGACTIVITIESINTAMIL/AMAZING-ACTIVITIES-IN-TAMIL/output-1-to-20.json, file:/home/shanks/setu/examples/sample_data/tamil/AMAZINGACTIVITIESINTAMIL/AMAZING-ACTIVITIES-IN-TAMIL/output-21-to-40.json, file:/home/shanks/setu/examples/sample_data/tamil/AMAZINGACTIVITIESINTAMIL/AMAZING-ACTIVITIES-IN-TAMIL/output-41-to-57.json, file:/home/shanks/setu/examples/sample_data/tamil/AMAZINGBONETAMIL/AMAZING_BONE_-_TAMIL/output-1-to-20.json, file:/home/shanks/setu/examples/sample_data/tamil/AMAZINGBONETAMIL/AMAZING_BONE_-_TAMIL/output-21-to-29.json, file:/home/shanks/setu/examples/sample_data/tamil/AgamumPuramum/AgamumPuramum/output-1-to-20.json, file:/home/shanks/setu/examples/sample_data/tamil/AgamumPuramum/AgamumPuramum/output-101-to-120.json, file:/home/shanks/setu/examples/sample_data/tamil/AgamumPuramum/AgamumPuramum/output-121-to-140.json.\n",
      "24/03/29 10:45:42 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "24/03/29 10:45:42 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 87 output partitions\n",
      "24/03/29 10:45:42 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "24/03/29 10:45:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/29 10:45:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/29 10:45:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/29 10:45:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 103.5 KiB, free 3.4 GiB)\n",
      "24/03/29 10:45:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 3.4 GiB)\n",
      "24/03/29 10:45:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:36475 (size: 37.3 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:45:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/29 10:45:42 INFO DAGScheduler: Submitting 87 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "24/03/29 10:45:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 87 tasks resource profile 0\n",
      "24/03/29 10:45:44 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:58180) with ID 1,  ResourceProfileId 0\n",
      "24/03/29 10:45:44 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:58192) with ID 0,  ResourceProfileId 0\n",
      "24/03/29 10:45:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.108.238:44597 with 1663.2 MiB RAM, BlockManagerId(0, 172.31.108.238, 44597, None)\n",
      "24/03/29 10:45:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.108.238:41919 with 1663.2 MiB RAM, BlockManagerId(1, 172.31.108.238, 41919, None)\n",
      "24/03/29 10:46:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.31.108.238, executor 0, partition 0, PROCESS_LOCAL, 8014 bytes) \n",
      "24/03/29 10:46:59 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.31.108.238, executor 0, partition 1, PROCESS_LOCAL, 8015 bytes) \n",
      "24/03/29 10:46:59 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.31.108.238, executor 1, partition 2, PROCESS_LOCAL, 8023 bytes) \n",
      "24/03/29 10:46:59 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.31.108.238, executor 1, partition 3, PROCESS_LOCAL, 8024 bytes) \n",
      "24/03/29 10:46:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.31.108.238:41919 (size: 37.3 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:46:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.31.108.238:44597 (size: 37.3 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.31.108.238, executor 1, partition 4, PROCESS_LOCAL, 8024 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.31.108.238, executor 0, partition 5, PROCESS_LOCAL, 8008 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.31.108.238, executor 0, partition 6, PROCESS_LOCAL, 8009 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.31.108.238, executor 1, partition 7, PROCESS_LOCAL, 7998 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 797 ms on 172.31.108.238 (executor 0) (1/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 762 ms on 172.31.108.238 (executor 1) (2/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 763 ms on 172.31.108.238 (executor 1) (3/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 784 ms on 172.31.108.238 (executor 0) (4/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.31.108.238, executor 0, partition 8, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 43 ms on 172.31.108.238 (executor 0) (5/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.31.108.238, executor 0, partition 9, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 32 ms on 172.31.108.238 (executor 0) (6/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.31.108.238, executor 1, partition 10, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (172.31.108.238, executor 1, partition 11, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 56 ms on 172.31.108.238 (executor 1) (7/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 34 ms on 172.31.108.238 (executor 1) (8/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (172.31.108.238, executor 0, partition 12, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 30 ms on 172.31.108.238 (executor 0) (9/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (172.31.108.238, executor 0, partition 13, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 28 ms on 172.31.108.238 (executor 0) (10/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (172.31.108.238, executor 1, partition 14, PROCESS_LOCAL, 7999 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (172.31.108.238, executor 1, partition 15, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 29 ms on 172.31.108.238 (executor 1) (11/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 28 ms on 172.31.108.238 (executor 1) (12/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (172.31.108.238, executor 0, partition 16, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (172.31.108.238, executor 0, partition 17, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 29 ms on 172.31.108.238 (executor 0) (13/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 27 ms on 172.31.108.238 (executor 0) (14/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (172.31.108.238, executor 1, partition 18, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (172.31.108.238, executor 1, partition 19, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 33 ms on 172.31.108.238 (executor 1) (15/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 37 ms on 172.31.108.238 (executor 1) (16/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (172.31.108.238, executor 0, partition 20, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 26 ms on 172.31.108.238 (executor 0) (17/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (172.31.108.238, executor 0, partition 21, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 31 ms on 172.31.108.238 (executor 0) (18/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (172.31.108.238, executor 1, partition 22, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 27 ms on 172.31.108.238 (executor 1) (19/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (172.31.108.238, executor 1, partition 23, PROCESS_LOCAL, 8001 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 27 ms on 172.31.108.238 (executor 1) (20/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (172.31.108.238, executor 0, partition 24, PROCESS_LOCAL, 7999 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 26 ms on 172.31.108.238 (executor 0) (21/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (172.31.108.238, executor 0, partition 25, PROCESS_LOCAL, 7999 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 25 ms on 172.31.108.238 (executor 0) (22/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (172.31.108.238, executor 1, partition 26, PROCESS_LOCAL, 8000 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 27 ms on 172.31.108.238 (executor 1) (23/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (172.31.108.238, executor 1, partition 27, PROCESS_LOCAL, 8024 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 28 ms on 172.31.108.238 (executor 1) (24/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (172.31.108.238, executor 0, partition 28, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (172.31.108.238, executor 0, partition 29, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 26 ms on 172.31.108.238 (executor 0) (25/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 23 ms on 172.31.108.238 (executor 0) (26/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (172.31.108.238, executor 1, partition 30, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 26 ms on 172.31.108.238 (executor 1) (27/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (172.31.108.238, executor 1, partition 31, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 25 ms on 172.31.108.238 (executor 1) (28/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (172.31.108.238, executor 0, partition 32, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (172.31.108.238, executor 0, partition 33, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 23 ms on 172.31.108.238 (executor 0) (29/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 25 ms on 172.31.108.238 (executor 0) (30/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (172.31.108.238, executor 1, partition 34, PROCESS_LOCAL, 8025 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 27 ms on 172.31.108.238 (executor 1) (31/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (172.31.108.238, executor 1, partition 35, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 26 ms on 172.31.108.238 (executor 1) (32/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 22 ms on 172.31.108.238 (executor 0) (33/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (172.31.108.238, executor 0, partition 36, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (172.31.108.238, executor 0, partition 37, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 25 ms on 172.31.108.238 (executor 0) (34/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (172.31.108.238, executor 0, partition 38, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 19 ms on 172.31.108.238 (executor 0) (35/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (172.31.108.238, executor 0, partition 39, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (172.31.108.238, executor 1, partition 40, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 24 ms on 172.31.108.238 (executor 0) (36/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 32 ms on 172.31.108.238 (executor 1) (37/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (172.31.108.238, executor 1, partition 41, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 33 ms on 172.31.108.238 (executor 1) (38/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (172.31.108.238, executor 1, partition 42, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (172.31.108.238, executor 1, partition 43, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 22 ms on 172.31.108.238 (executor 1) (39/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 25 ms on 172.31.108.238 (executor 1) (40/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (172.31.108.238, executor 0, partition 44, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 35 ms on 172.31.108.238 (executor 0) (41/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (172.31.108.238, executor 0, partition 45, PROCESS_LOCAL, 8025 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 33 ms on 172.31.108.238 (executor 0) (42/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (172.31.108.238, executor 1, partition 46, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (172.31.108.238, executor 1, partition 47, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 23 ms on 172.31.108.238 (executor 1) (43/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (172.31.108.238, executor 0, partition 48, PROCESS_LOCAL, 8025 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 22 ms on 172.31.108.238 (executor 1) (44/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (172.31.108.238, executor 0, partition 49, PROCESS_LOCAL, 8026 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 21 ms on 172.31.108.238 (executor 0) (45/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 24 ms on 172.31.108.238 (executor 0) (46/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (172.31.108.238, executor 1, partition 50, PROCESS_LOCAL, 8036 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 18 ms on 172.31.108.238 (executor 1) (47/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (172.31.108.238, executor 1, partition 51, PROCESS_LOCAL, 8037 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (172.31.108.238, executor 0, partition 52, PROCESS_LOCAL, 8037 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 23 ms on 172.31.108.238 (executor 1) (48/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (172.31.108.238, executor 0, partition 53, PROCESS_LOCAL, 8037 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 23 ms on 172.31.108.238 (executor 0) (49/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 21 ms on 172.31.108.238 (executor 0) (50/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (172.31.108.238, executor 1, partition 54, PROCESS_LOCAL, 8032 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 21 ms on 172.31.108.238 (executor 1) (51/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (172.31.108.238, executor 1, partition 55, PROCESS_LOCAL, 8033 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 21 ms on 172.31.108.238 (executor 1) (52/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (172.31.108.238, executor 0, partition 56, PROCESS_LOCAL, 8033 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (172.31.108.238, executor 0, partition 57, PROCESS_LOCAL, 8033 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 23 ms on 172.31.108.238 (executor 0) (53/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 22 ms on 172.31.108.238 (executor 0) (54/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (172.31.108.238, executor 1, partition 58, PROCESS_LOCAL, 8033 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (172.31.108.238, executor 1, partition 59, PROCESS_LOCAL, 8015 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 22 ms on 172.31.108.238 (executor 1) (55/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 20 ms on 172.31.108.238 (executor 1) (56/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (172.31.108.238, executor 0, partition 60, PROCESS_LOCAL, 8016 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 19 ms on 172.31.108.238 (executor 0) (57/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (172.31.108.238, executor 0, partition 61, PROCESS_LOCAL, 8002 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 20 ms on 172.31.108.238 (executor 0) (58/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (172.31.108.238, executor 1, partition 62, PROCESS_LOCAL, 8003 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (172.31.108.238, executor 1, partition 63, PROCESS_LOCAL, 8024 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (172.31.108.238, executor 0, partition 64, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 22 ms on 172.31.108.238 (executor 0) (59/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 27 ms on 172.31.108.238 (executor 1) (60/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (172.31.108.238, executor 0, partition 65, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 34 ms on 172.31.108.238 (executor 1) (61/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 32 ms on 172.31.108.238 (executor 0) (62/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (172.31.108.238, executor 0, partition 66, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 21 ms on 172.31.108.238 (executor 0) (63/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 23 ms on 172.31.108.238 (executor 1) (64/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (172.31.108.238, executor 1, partition 67, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (172.31.108.238, executor 1, partition 68, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (172.31.108.238, executor 0, partition 69, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 30 ms on 172.31.108.238 (executor 1) (65/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 21 ms on 172.31.108.238 (executor 0) (66/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (172.31.108.238, executor 0, partition 70, PROCESS_LOCAL, 8025 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 19 ms on 172.31.108.238 (executor 0) (67/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (172.31.108.238, executor 0, partition 71, PROCESS_LOCAL, 8027 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 19 ms on 172.31.108.238 (executor 0) (68/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (172.31.108.238, executor 1, partition 72, PROCESS_LOCAL, 8025 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (172.31.108.238, executor 1, partition 73, PROCESS_LOCAL, 8025 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 25 ms on 172.31.108.238 (executor 1) (69/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 28 ms on 172.31.108.238 (executor 1) (70/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (172.31.108.238, executor 0, partition 74, PROCESS_LOCAL, 8026 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 21 ms on 172.31.108.238 (executor 0) (71/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (172.31.108.238, executor 0, partition 75, PROCESS_LOCAL, 8042 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 21 ms on 172.31.108.238 (executor 0) (72/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (172.31.108.238, executor 0, partition 76, PROCESS_LOCAL, 8043 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (172.31.108.238, executor 1, partition 77, PROCESS_LOCAL, 8020 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (172.31.108.238, executor 1, partition 78, PROCESS_LOCAL, 8021 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 39 ms on 172.31.108.238 (executor 1) (73/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 37 ms on 172.31.108.238 (executor 1) (74/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 29 ms on 172.31.108.238 (executor 0) (75/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (172.31.108.238, executor 0, partition 79, PROCESS_LOCAL, 8085 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 23 ms on 172.31.108.238 (executor 0) (76/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (172.31.108.238, executor 0, partition 80, PROCESS_LOCAL, 8086 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 31 ms on 172.31.108.238 (executor 0) (77/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (172.31.108.238, executor 1, partition 81, PROCESS_LOCAL, 8029 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (172.31.108.238, executor 1, partition 82, PROCESS_LOCAL, 8030 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 34 ms on 172.31.108.238 (executor 1) (78/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 31 ms on 172.31.108.238 (executor 1) (79/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (172.31.108.238, executor 0, partition 83, PROCESS_LOCAL, 8032 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 31 ms on 172.31.108.238 (executor 0) (80/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (172.31.108.238, executor 0, partition 84, PROCESS_LOCAL, 8033 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 22 ms on 172.31.108.238 (executor 0) (81/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 20 ms on 172.31.108.238 (executor 1) (82/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (172.31.108.238, executor 1, partition 85, PROCESS_LOCAL, 8033 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (172.31.108.238, executor 1, partition 86, PROCESS_LOCAL, 8033 bytes) \n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 22 ms on 172.31.108.238 (executor 1) (83/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 25 ms on 172.31.108.238 (executor 0) (84/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 19 ms on 172.31.108.238 (executor 0) (85/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 18 ms on 172.31.108.238 (executor 1) (86/87)\n",
      "24/03/29 10:47:00 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 18 ms on 172.31.108.238 (executor 1) (87/87)\n",
      "24/03/29 10:47:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/29 10:47:00 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 78.400 s\n",
      "24/03/29 10:47:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/29 10:47:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/03/29 10:47:00 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 78.442873 s\n",
      "24/03/29 10:47:00 INFO InMemoryFileIndex: It took 78801 ms to list leaf files for 87 paths.\n",
      "24/03/29 10:47:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 211.8 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.8 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:36475 (size: 36.8 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:01 INFO SparkContext: Created broadcast 1 from load at NativeMethodAccessorImpl.java:0\n",
      "24/03/29 10:47:01 INFO FileInputFormat: Total input files to process : 87\n",
      "24/03/29 10:47:01 INFO FileInputFormat: Total input files to process : 87\n",
      "24/03/29 10:47:01 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "24/03/29 10:47:01 INFO DAGScheduler: Got job 1 (load at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/03/29 10:47:01 INFO DAGScheduler: Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)\n",
      "24/03/29 10:47:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/29 10:47:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/29 10:47:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/29 10:47:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.3 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:36475 (size: 5.0 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/29 10:47:01 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/29 10:47:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
      "24/03/29 10:47:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 87) (172.31.108.238, executor 1, partition 0, PROCESS_LOCAL, 19614 bytes) \n",
      "24/03/29 10:47:01 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 88) (172.31.108.238, executor 0, partition 1, PROCESS_LOCAL, 8162 bytes) \n",
      "24/03/29 10:47:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:36475 in memory (size: 37.3 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.31.108.238:44597 in memory (size: 37.3 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.31.108.238:41919 in memory (size: 37.3 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.31.108.238:44597 (size: 5.0 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.31.108.238:41919 (size: 5.0 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.31.108.238:44597 (size: 36.8 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.31.108.238:41919 (size: 36.8 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:01 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 88) in 437 ms on 172.31.108.238 (executor 0) (1/2)\n",
      "24/03/29 10:47:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 87) in 2496 ms on 172.31.108.238 (executor 1) (2/2)\n",
      "24/03/29 10:47:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/03/29 10:47:03 INFO DAGScheduler: ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 2.511 s\n",
      "24/03/29 10:47:03 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/29 10:47:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/03/29 10:47:03 INFO DAGScheduler: Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 2.520020 s\n",
      "24/03/29 10:47:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:36475 in memory (size: 36.8 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.31.108.238:44597 in memory (size: 36.8 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.31.108.238:41919 in memory (size: 36.8 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:36475 in memory (size: 5.0 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.31.108.238:41919 in memory (size: 5.0 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.31.108.238:44597 in memory (size: 5.0 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(responses)\n",
      "24/03/29 10:47:05 INFO FileSourceStrategy: Post-Scan Filters: (size(responses#1, true) > 0),isnotnull(responses#1)\n",
      "24/03/29 10:47:05 INFO CodeGenerator: Code generated in 193.096558 ms\n",
      "24/03/29 10:47:06 INFO CodeGenerator: Code generated in 184.027594 ms\n",
      "24/03/29 10:47:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 201.0 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:36475 (size: 34.7 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:06 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/29 10:47:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 122383142 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/29 10:47:06 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/29 10:47:06 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/29 10:47:06 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/29 10:47:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/29 10:47:06 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/29 10:47:06 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/29 10:47:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 322.4 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 81.4 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:36475 (size: 81.4 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:06 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/29 10:47:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/29 10:47:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/03/29 10:47:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 89) (172.31.108.238, executor 1, partition 0, PROCESS_LOCAL, 11800 bytes) \n",
      "24/03/29 10:47:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.31.108.238:41919 (size: 81.4 KiB, free: 1663.1 MiB)\n",
      "24/03/29 10:47:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.31.108.238:41919 (size: 34.7 KiB, free: 1663.1 MiB)\n",
      "24/03/29 10:47:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 89) in 2171 ms on 172.31.108.238 (executor 1) (1/1)\n",
      "24/03/29 10:47:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/03/29 10:47:08 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 44357\n",
      "24/03/29 10:47:08 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 2.188 s\n",
      "24/03/29 10:47:08 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/29 10:47:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/03/29 10:47:08 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 2.193863 s\n",
      "24/03/29 10:47:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:36475 in memory (size: 81.4 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.31.108.238:41919 in memory (size: 81.4 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:09 INFO CodeGenerator: Code generated in 9.370991 ms\n",
      "+--------------------+---------------+----------+--------------------+---------------+-----+------+------------+---------------+\n",
      "|                 uri|       mimeType|pageNumber|              blocks|page_confidence|width|height|languageCode|lang_confidence|\n",
      "+--------------------+---------------+----------+--------------------+---------------+-----+------+------------+---------------+\n",
      "|gs://sangraha_pdf...|application/pdf|       301|[{TEXT, {[{0.0997...|     0.98362637|  391|   581|          ta|            1.0|\n",
      "|gs://sangraha_pdf...|application/pdf|       302|[{TEXT, {[{0.1224...|     0.87699753|  392|   579|          ta|      0.9959199|\n",
      "|gs://sangraha_pdf...|application/pdf|       303|[{TEXT, {[{0.0956...|      0.6963466|  387|   580|          ta|      0.9927908|\n",
      "|gs://sangraha_pdf...|application/pdf|       304|[{TEXT, {[{0.1395...|       0.884997|  387|   579|          ta|      0.9941771|\n",
      "|gs://sangraha_pdf...|application/pdf|       305|[{TEXT, {[{0.8158...|      0.8537299|  391|   577|          ta|     0.96953714|\n",
      "+--------------------+---------------+----------+--------------------+---------------+-----+------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "24/03/29 10:47:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(responses)\n",
      "24/03/29 10:47:09 INFO FileSourceStrategy: Post-Scan Filters: (size(responses#1, true) > 0),isnotnull(responses#1)\n",
      "24/03/29 10:47:09 INFO CodeGenerator: Code generated in 71.795042 ms\n",
      "24/03/29 10:47:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 201.0 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:36475 (size: 34.7 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:09 INFO SparkContext: Created broadcast 5 from count at NativeMethodAccessorImpl.java:0\n",
      "24/03/29 10:47:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 122383142 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/29 10:47:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:36475 in memory (size: 34.7 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.31.108.238:41919 in memory (size: 34.7 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:09 INFO DAGScheduler: Registering RDD 15 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/03/29 10:47:09 INFO DAGScheduler: Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/03/29 10:47:09 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/29 10:47:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/29 10:47:09 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/29 10:47:09 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/29 10:47:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 141.2 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:36475 (size: 36.6 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:09 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/29 10:47:09 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "24/03/29 10:47:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 4 tasks resource profile 0\n",
      "24/03/29 10:47:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 90) (172.31.108.238, executor 0, partition 0, PROCESS_LOCAL, 11789 bytes) \n",
      "24/03/29 10:47:09 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 91) (172.31.108.238, executor 1, partition 1, PROCESS_LOCAL, 11818 bytes) \n",
      "24/03/29 10:47:09 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 92) (172.31.108.238, executor 0, partition 2, PROCESS_LOCAL, 12278 bytes) \n",
      "24/03/29 10:47:09 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 93) (172.31.108.238, executor 1, partition 3, PROCESS_LOCAL, 12863 bytes) \n",
      "24/03/29 10:47:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.31.108.238:44597 (size: 36.6 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.31.108.238:41919 (size: 36.6 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.31.108.238:41919 (size: 34.7 KiB, free: 1663.1 MiB)\n",
      "24/03/29 10:47:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.31.108.238:44597 (size: 34.7 KiB, free: 1663.1 MiB)\n",
      "24/03/29 10:47:11 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 93) in 2011 ms on 172.31.108.238 (executor 1) (1/4)\n",
      "24/03/29 10:47:12 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 92) in 2648 ms on 172.31.108.238 (executor 0) (2/4)\n",
      "24/03/29 10:47:12 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 91) in 2807 ms on 172.31.108.238 (executor 1) (3/4)\n",
      "24/03/29 10:47:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 90) in 2963 ms on 172.31.108.238 (executor 0) (4/4)\n",
      "24/03/29 10:47:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/03/29 10:47:12 INFO DAGScheduler: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 2.982 s\n",
      "24/03/29 10:47:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/29 10:47:12 INFO DAGScheduler: running: Set()\n",
      "24/03/29 10:47:12 INFO DAGScheduler: waiting: Set()\n",
      "24/03/29 10:47:12 INFO DAGScheduler: failed: Set()\n",
      "24/03/29 10:47:12 INFO CodeGenerator: Code generated in 6.122342 ms\n",
      "24/03/29 10:47:12 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/03/29 10:47:12 INFO DAGScheduler: Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/29 10:47:12 INFO DAGScheduler: Final stage: ResultStage 5 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/29 10:47:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "24/03/29 10:47:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/29 10:47:12 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/29 10:47:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.5 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:36475 (size: 5.9 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:36475 in memory (size: 36.6 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/29 10:47:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/29 10:47:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/03/29 10:47:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.31.108.238:44597 in memory (size: 36.6 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.31.108.238:41919 in memory (size: 36.6 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 94) (172.31.108.238, executor 1, partition 0, NODE_LOCAL, 7784 bytes) \n",
      "24/03/29 10:47:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.31.108.238:41919 (size: 5.9 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 127.0.0.1:58180\n",
      "24/03/29 10:47:12 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 94) in 132 ms on 172.31.108.238 (executor 1) (1/1)\n",
      "24/03/29 10:47:12 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/03/29 10:47:12 INFO DAGScheduler: ResultStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.148 s\n",
      "24/03/29 10:47:12 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/29 10:47:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/03/29 10:47:12 INFO DAGScheduler: Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.157330 s\n",
      "24/03/29 10:47:12 INFO FileSourceStrategy: Pushed Filters: IsNotNull(responses)\n",
      "24/03/29 10:47:12 INFO FileSourceStrategy: Post-Scan Filters: (size(responses#1, true) > 0),isnotnull(responses#1)\n",
      "24/03/29 10:47:12 INFO CodeGenerator: Code generated in 20.070645 ms\n",
      "24/03/29 10:47:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:36475 in memory (size: 5.9 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.31.108.238:41919 in memory (size: 5.9 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:13 INFO CodeGenerator: Code generated in 116.939544 ms\n",
      "24/03/29 10:47:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.0 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:36475 (size: 34.7 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:13 INFO SparkContext: Created broadcast 8 from parquet at NativeMethodAccessorImpl.java:0\n",
      "24/03/29 10:47:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 122383142 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/29 10:47:13 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:36475 in memory (size: 34.7 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:13 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.31.108.238:44597 in memory (size: 34.7 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:13 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.31.108.238:41919 in memory (size: 34.7 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:13 INFO DAGScheduler: Registering RDD 25 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "24/03/29 10:47:13 INFO DAGScheduler: Got map stage job 5 (parquet at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/03/29 10:47:13 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "24/03/29 10:47:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/29 10:47:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/29 10:47:13 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[25] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/29 10:47:13 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 297.7 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:13 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:36475 (size: 76.3 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:13 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/29 10:47:13 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[25] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "24/03/29 10:47:13 INFO TaskSchedulerImpl: Adding task set 6.0 with 4 tasks resource profile 0\n",
      "24/03/29 10:47:13 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 95) (172.31.108.238, executor 0, partition 0, PROCESS_LOCAL, 11789 bytes) \n",
      "24/03/29 10:47:13 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 96) (172.31.108.238, executor 1, partition 1, PROCESS_LOCAL, 11818 bytes) \n",
      "24/03/29 10:47:13 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 97) (172.31.108.238, executor 0, partition 2, PROCESS_LOCAL, 12278 bytes) \n",
      "24/03/29 10:47:13 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 98) (172.31.108.238, executor 1, partition 3, PROCESS_LOCAL, 12863 bytes) \n",
      "24/03/29 10:47:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.31.108.238:44597 (size: 76.3 KiB, free: 1663.1 MiB)\n",
      "24/03/29 10:47:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.31.108.238:41919 (size: 76.3 KiB, free: 1663.1 MiB)\n",
      "24/03/29 10:47:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.31.108.238:41919 (size: 34.7 KiB, free: 1663.1 MiB)\n",
      "24/03/29 10:47:14 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.31.108.238:44597 (size: 34.7 KiB, free: 1663.1 MiB)\n",
      "24/03/29 10:47:14 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 98) in 1201 ms on 172.31.108.238 (executor 1) (1/4)\n",
      "24/03/29 10:47:15 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 96) in 2020 ms on 172.31.108.238 (executor 1) (2/4)\n",
      "24/03/29 10:47:15 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 97) in 2543 ms on 172.31.108.238 (executor 0) (3/4)\n",
      "24/03/29 10:47:15 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 95) in 2855 ms on 172.31.108.238 (executor 0) (4/4)\n",
      "24/03/29 10:47:15 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/03/29 10:47:15 INFO DAGScheduler: ShuffleMapStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.866 s\n",
      "24/03/29 10:47:15 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/29 10:47:15 INFO DAGScheduler: running: Set()\n",
      "24/03/29 10:47:15 INFO DAGScheduler: waiting: Set()\n",
      "24/03/29 10:47:15 INFO DAGScheduler: failed: Set()\n",
      "24/03/29 10:47:16 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/03/29 10:47:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/03/29 10:47:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/03/29 10:47:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/03/29 10:47:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/03/29 10:47:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/03/29 10:47:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/03/29 10:47:16 INFO CodeGenerator: Code generated in 22.96297 ms\n",
      "24/03/29 10:47:16 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "24/03/29 10:47:16 INFO DAGScheduler: Got job 6 (parquet at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/03/29 10:47:16 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "24/03/29 10:47:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "24/03/29 10:47:16 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/29 10:47:16 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[28] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/29 10:47:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:36475 in memory (size: 76.3 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.31.108.238:41919 in memory (size: 76.3 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.31.108.238:44597 in memory (size: 76.3 KiB, free: 1663.2 MiB)\n",
      "24/03/29 10:47:16 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 244.4 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:16 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 84.1 KiB, free 3.4 GiB)\n",
      "24/03/29 10:47:16 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:36475 (size: 84.1 KiB, free: 3.4 GiB)\n",
      "24/03/29 10:47:16 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/29 10:47:16 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[28] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/29 10:47:16 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0\n",
      "24/03/29 10:47:16 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 99) (172.31.108.238, executor 0, partition 1, NODE_LOCAL, 7784 bytes) \n",
      "24/03/29 10:47:16 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 100) (172.31.108.238, executor 0, partition 0, PROCESS_LOCAL, 7784 bytes) \n",
      "24/03/29 10:47:16 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.31.108.238:44597 (size: 84.1 KiB, free: 1663.1 MiB)\n",
      "24/03/29 10:47:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 127.0.0.1:58192\n",
      "24/03/29 10:47:16 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 100) in 662 ms on 172.31.108.238 (executor 0) (1/2)\n",
      "24/03/29 10:47:17 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 99) in 1815 ms on 172.31.108.238 (executor 0) (2/2)\n",
      "24/03/29 10:47:17 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/03/29 10:47:17 INFO DAGScheduler: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.852 s\n",
      "24/03/29 10:47:17 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/29 10:47:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/03/29 10:47:17 INFO DAGScheduler: Job 6 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.858071 s\n",
      "24/03/29 10:47:17 INFO FileFormatWriter: Start to commit write Job c3b3b8a7-f6d1-47ec-a2ce-eefed1976288.\n",
      "24/03/29 10:47:17 INFO FileFormatWriter: Write Job c3b3b8a7-f6d1-47ec-a2ce-eefed1976288 committed. Elapsed time: 19 ms.\n",
      "24/03/29 10:47:17 INFO FileFormatWriter: Finished processing stats for write job c3b3b8a7-f6d1-47ec-a2ce-eefed1976288.\n",
      "24/03/29 10:47:18 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "24/03/29 10:47:18 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/03/29 10:47:18 INFO SparkUI: Stopped Spark web UI at http://localhost:4041\n",
      "24/03/29 10:47:18 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/03/29 10:47:18 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/03/29 10:47:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/03/29 10:47:18 INFO MemoryStore: MemoryStore cleared\n",
      "24/03/29 10:47:18 INFO BlockManager: BlockManager stopped\n",
      "24/03/29 10:47:18 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/03/29 10:47:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/03/29 10:47:18 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/03/29 10:47:18 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/03/29 10:47:18 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-def91191-96f1-43f8-bbe8-630c36984b07\n",
      "24/03/29 10:47:18 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-3cafdb11-916e-4092-aa99-1b1c55818a8a/pyspark-dae66f18-4632-4638-8572-608d9938d537\n",
      "24/03/29 10:47:18 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-3cafdb11-916e-4092-aa99-1b1c55818a8a\n",
      "24/03/29 10:47:18 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-00d2f8ea-e74c-4e4d-b295-f9456527d81a\n"
     ]
    }
   ],
   "source": [
    "!SETU_DIR=/home/$USER/setu SETU_TMP_DIR=/home/$USER/tmp/ FILTER_DATA_ROOT=/home/$USER/setu/setu/data \\\n",
    "    spark-submit \\\n",
    "    --master spark://YDEARYZEN.:7077 \\\n",
    "    --deploy-mode client \\\n",
    "    --driver-java-options -Djava.io.tmpdir=/home/$USER/tmp/ \\\n",
    "    --conf \"spark.driver.extraJavaOptions=-Djava.io.tmpdir=/home/$USER/tmp/\" \\\n",
    "    --conf \"spark.executor.extraJavaOptions=-Djava.io.tmpdir=/home/$USER/tmp/\" \\\n",
    "    --conf spark.worker.dir=\"/home/$USER/tmp/\" \\\n",
    "    --conf spark.local.dir=\"/home/$USER/tmp/\" \\\n",
    "    --num-executors 4 \\\n",
    "    --executor-cores 2 \\\n",
    "    --executor-memory 3G \\\n",
    "    --driver-memory 6G \\\n",
    "    --archives \"/home/$USER/setu/dataproc/envs/setu.zip\" \\\n",
    "    --conf 'spark.executorEnv.PYTHONPATH=setu.zip' \\\n",
    "    --conf 'spark.executorEnv.FILTER_DATA_ROOT=setu.zip/data' \\\n",
    "    /home/$USER/setu/setu/run.py \\\n",
    "    --config /home/$USER/setu/configs/ocr/spark_tamil_config.json \\\n",
    "    --mode ocr \\\n",
    "    --run_local True \\\n",
    "    FormatConversionStage \\\n",
    "    --fc_json_glob \"/home/shanks/setu/examples/sample_data/tamil/*/*/*.json\" \\\n",
    "    --fc_samples_per_partition 1500 \\\n",
    "    --fc_run_mode \"stage\" \\\n",
    "    --fc_output_path \"/home/$USER/setu/examples/output/fc_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/08 07:03:02 WARN Utils: Your hostname, YDEARYZEN resolves to a loopback address: 127.0.1.1; using 172.31.108.238 instead (on interface eth0)\n",
      "24/04/08 07:03:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/04/08 07:03:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "shanks\n",
      "------------------------------------------------ Setting Environment Variables --------------------------------------------------\n",
      "------------------------------------------------ END --------------------------------------------------\n",
      "------------------------------------------------ Environment Variables --------------------------------------------------\n",
      "PATH: /home/shanks/miniconda3/envs/setu_env/bin:/home/shanks/.vscode-server/bin/5c3e652f63e798a5ac2f31ffd0d863669328dc4c/bin/remote-cli:/home/shanks/hadoop/spark-3.5.1/bin:/home/shanks/miniconda3/bin:/home/shanks/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files (x86)/VMware/VMware Player/bin:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/dotnet:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files/Go/bin:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/RabbitMQ Server/rabbitmq_server-3.12.13/sbin:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311/Scripts:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311:/mnt/c/Users/Shanks/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Shanks/AppData/Roaming/npm:/mnt/c/Users/Shanks/AppData/Local/Google/Cloud SDK/google-cloud-sdk/bin:/mnt/c/Users/Shanks/miniconda3/Scripts:/mnt/c/Users/Shanks/miniconda3:/mnt/c/Users/Shanks/miniconda3/Library/bin/conda.bat:/snap/bin:/home/hadoop/hadoop-3.4.0/sbin:/home/hadoop/hadoop-3.4.0/bin:/home/shanks/.vscode-server/bin/5c3e652f63e798a5ac2f31ffd0d863669328dc4c/bin/remote-cli:/home/shanks/hadoop/spark-3.5.1/bin:/home/shanks/miniconda3/bin:/home/shanks/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files (x86)/VMware/VMware Player/bin:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/dotnet:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files/Go/bin:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/RabbitMQ Server/rabbitmq_server-3.12.13/sbin:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311/Scripts:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311:/mnt/c/Users/Shanks/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Shanks/AppData/Roaming/npm:/mnt/c/Users/Shanks/AppData/Local/Google/Cloud SDK/google-cloud-sdk/bin:/mnt/c/Users/Shanks/miniconda3/Scripts:/mnt/c/Users/Shanks/miniconda3:/mnt/c/Users/Shanks/miniconda3/Library/bin/conda.bat:/snap/bin:/home/hadoop/hadoop-3.4.0/sbin:/home/hadoop/hadoop-3.4.0/bin\n",
      "XDG_DATA_DIRS: /usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "CONDA_DEFAULT_ENV: base\n",
      "GIT_PAGER: cat\n",
      "CLICOLOR_FORCE: 1\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "SPARK_ENV_LOADED: 1\n",
      "SPARK_CLASSPATH: :/path/to/your/hadoop-lzo/java/libs\n",
      "CONDA_PYTHON_EXE: /home/shanks/miniconda3/bin/python\n",
      "WSL2_GUI_APPS_ENABLED: 1\n",
      "DBUS_SESSION_BUS_ADDRESS: unix:path=/run/user/1000/bus\n",
      "FILTER_DATA_ROOT: /home/shanks/setu/setu/data\n",
      "CONDA_PREFIX: /home/shanks/miniconda3\n",
      "LOGNAME: shanks\n",
      "SETU_TMP_DIR: /home/shanks/tmp/\n",
      "PWD: /home/shanks/setu/examples\n",
      "VSCODE_NLS_CONFIG: {\"locale\":\"en\",\"osLocale\":\"en\",\"availableLanguages\":{}}\n",
      "HADOOP_INSTALL: /home/hadoop/hadoop-3.4.0\n",
      "PYTHONPATH: /home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip:/home/shanks/hadoop/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip:/home/shanks/hadoop/spark-3.5.1/jars/spark-core_2.12-3.5.1.jar\n",
      "LESSOPEN: | /usr/bin/lesspipe %s\n",
      "SHELL: /bin/bash\n",
      "WSL_INTEROP: /run/WSL/395_interop\n",
      "PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "HOSTTYPE: x86_64\n",
      "HADOOP_YARN_HOME: /home/hadoop/hadoop-3.4.0\n",
      "VSCODE_WSL_EXT_LOCATION: /mnt/c/Users/Shanks/.vscode/extensions/ms-vscode-remote.remote-wsl-0.88.0\n",
      "SPARK_CONF_DIR: /home/shanks/hadoop/spark-3.5.1/conf\n",
      "NAME: YDEARYZEN\n",
      "HADOOP_HOME: /home/hadoop/hadoop-3.4.0\n",
      "CONDA_PROMPT_MODIFIER: (base) \n",
      "LS_COLORS: rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\n",
      "SHLVL: 0\n",
      "SETU_DIR: /home/shanks/setu\n",
      "VSCODE_HANDLES_SIGPIPE: true\n",
      "LESSCLOSE: /usr/bin/lesspipe %s %s\n",
      "FORCE_COLOR: 1\n",
      "VSCODE_L10N_BUNDLE_LOCATION: \n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "CONDA_EXE: /home/shanks/miniconda3/bin/conda\n",
      "TERM: xterm-color\n",
      "SPARK_LIBRARY_PATH: :/path/to/your/hadoop-lzo/libs/native\n",
      "LANG: C.UTF-8\n",
      "SPARK_SCALA_VERSION: 2.12\n",
      "DISPLAY: :0\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "SPARK_HOME: /home/shanks/hadoop/spark-3.5.1\n",
      "_CE_M: \n",
      "WAYLAND_DISPLAY: wayland-0\n",
      "VSCODE_IPC_HOOK_CLI: /run/user/1000/vscode-ipc-97e77f94-2143-407b-8d93-58cf81368b7c.sock\n",
      "VSCODE_AMD_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "HADOOP_HDFS_HOME: /home/hadoop/hadoop-3.4.0\n",
      "HADOOP_MAPRED_HOME: /home/hadoop/hadoop-3.4.0\n",
      "HADOOP_COMMON_HOME: /home/hadoop/hadoop-3.4.0\n",
      "CONDA_SHLVL: 1\n",
      "PYTHONHASHSEED: 0\n",
      "VSCODE_CWD: /mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code\n",
      "WSL_DISTRO_NAME: Ubuntu\n",
      "PYTHONIOENCODING: utf-8\n",
      "HADOOP_OPTS: ”-Djava.library.path=/home/hadoop/hadoop-3.4.0/lib/native”\n",
      "PYSPARK_GATEWAY_SECRET: db1e19a7a1f575789832bd97fdeb03e61c67a794ebd041b00bbb5e48384a9c3d\n",
      "USER: shanks\n",
      "CLICOLOR: 1\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "PULSE_SERVER: unix:/mnt/wslg/PulseServer\n",
      "PYSPARK_GATEWAY_PORT: 33375\n",
      "_CE_CONDA: \n",
      "PYTHONUNBUFFERED: YES\n",
      "WSLENV: VSCODE_WSL_EXT_LOCATION/up\n",
      "HADOOP_COMMON_LIB_NATIVE_DIR: /home/hadoop/hadoop-3.4.0/lib/native\n",
      "XDG_RUNTIME_DIR: /run/user/1000/\n",
      "HOME: /home/shanks\n",
      "------------------------------------------------ END --------------------------------------------------\n",
      "------------------------------------------------ Directory Structure --------------------------------------------------\n",
      "commands.md\n",
      "crawl_demo.ipynb\n",
      "ocr_demo.ipynb\n",
      "output\n",
      "sample_data\n",
      "ls: cannot access 'setu.zip': No such file or directory\n",
      "------------------------------------------------ End --------------------------------------------------\n",
      "Command used to run this script:  /home/shanks/setu/setu/run.py --config /home/shanks/setu/configs/ocr/spark_tamil_config.json --mode ocr --run_local True PageAnalysisStage\n",
      "Entered Commandline Arguments:  Namespace(config='/home/shanks/setu/configs/ocr/spark_tamil_config.json', mode='ocr', run_local=True, stage='PageAnalysisStage', pa_parquets_path=None, pa_samples_per_partition=1500, pa_run_mode='stage', pa_output_path=None)\n",
      "PySpark Installed. Going forward with spark execution for JSON2ParquetStage\n",
      "PySpark Installed. Going forward with spark execution for ExtractTextStage\n",
      "PySpark Installed. Going forward with spark execution for DocCleanStage\n",
      "PySpark Installed. Going forward with spark execution for LIDStage\n",
      "PySpark Installed. Going forward with spark execution for AnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for FlaggingAndFilteringStage\n",
      "PySpark Installed. Going forward with spark execution for DocumentRemovalStage\n",
      "PySpark Installed. Going forward with spark execution for FormatConversionStage\n",
      "PySpark Installed. Going forward with spark execution for PageAnalysisStage\n",
      "24/04/08 07:03:08 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/04/08 07:03:08 INFO SparkContext: OS info Linux, 5.15.133.1-microsoft-standard-WSL2, amd64\n",
      "24/04/08 07:03:08 INFO SparkContext: Java version 11.0.22\n",
      "24/04/08 07:03:08 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/04/08 07:03:08 INFO ResourceUtils: ==============================================================\n",
      "24/04/08 07:03:08 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/04/08 07:03:08 INFO ResourceUtils: ==============================================================\n",
      "24/04/08 07:03:08 INFO SparkContext: Submitted application: Sangraha Anaylsis\n",
      "24/04/08 07:03:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/04/08 07:03:08 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "24/04/08 07:03:08 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/04/08 07:03:08 INFO SecurityManager: Changing view acls to: shanks\n",
      "24/04/08 07:03:08 INFO SecurityManager: Changing modify acls to: shanks\n",
      "24/04/08 07:03:08 INFO SecurityManager: Changing view acls groups to: \n",
      "24/04/08 07:03:08 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/04/08 07:03:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: shanks; groups with view permissions: EMPTY; users with modify permissions: shanks; groups with modify permissions: EMPTY\n",
      "24/04/08 07:03:08 INFO Utils: Successfully started service 'sparkDriver' on port 33625.\n",
      "24/04/08 07:03:08 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/04/08 07:03:08 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/04/08 07:03:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/04/08 07:03:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/04/08 07:03:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/04/08 07:03:08 INFO DiskBlockManager: Created local directory at /home/shanks/tmp/blockmgr-a899169e-5951-4f34-be41-c0d90e733bfb\n",
      "24/04/08 07:03:08 INFO MemoryStore: MemoryStore started with capacity 3.4 GiB\n",
      "24/04/08 07:03:08 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/04/08 07:03:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/04/08 07:03:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/04/08 07:03:08 INFO SparkContext: Added archive file:///home/shanks/setu/dataproc/envs/setu.zip at spark://localhost:33625/files/setu.zip with timestamp 1712539988041\n",
      "24/04/08 07:03:08 INFO Utils: Copying /home/shanks/setu/dataproc/envs/setu.zip to /home/shanks/tmp/spark-92ede60f-2247-4548-b1b5-57dafa9711d2/setu.zip\n",
      "24/04/08 07:03:13 INFO SparkContext: Unpacking an archive file:///home/shanks/setu/dataproc/envs/setu.zip from /home/shanks/tmp/spark-92ede60f-2247-4548-b1b5-57dafa9711d2/setu.zip to /home/shanks/tmp/spark-1793a544-fc91-4c71-943d-df298e262bc4/userFiles-4b254ac4-283d-457a-8ad4-ad47df221c18/setu.zip\n",
      "24/04/08 07:03:26 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://YDEARYZEN.:7077...\n",
      "24/04/08 07:03:26 INFO TransportClientFactory: Successfully created connection to YDEARYZEN./127.0.1.1:7077 after 33 ms (0 ms spent in bootstraps)\n",
      "24/04/08 07:03:26 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240408070326-0001\n",
      "24/04/08 07:03:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240408070326-0001/0 on worker-20240408070148-172.31.108.238-43095 (172.31.108.238:43095) with 2 core(s)\n",
      "24/04/08 07:03:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20240408070326-0001/0 on hostPort 172.31.108.238:43095 with 2 core(s), 3.0 GiB RAM\n",
      "24/04/08 07:03:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240408070326-0001/1 on worker-20240408070148-172.31.108.238-43095 (172.31.108.238:43095) with 2 core(s)\n",
      "24/04/08 07:03:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20240408070326-0001/1 on hostPort 172.31.108.238:43095 with 2 core(s), 3.0 GiB RAM\n",
      "24/04/08 07:03:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40889.\n",
      "24/04/08 07:03:26 INFO NettyBlockTransferService: Server created on localhost:40889\n",
      "24/04/08 07:03:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/04/08 07:03:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 40889, None)\n",
      "24/04/08 07:03:26 INFO BlockManagerMasterEndpoint: Registering block manager localhost:40889 with 3.4 GiB RAM, BlockManagerId(driver, localhost, 40889, None)\n",
      "24/04/08 07:03:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 40889, None)\n",
      "24/04/08 07:03:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 40889, None)\n",
      "24/04/08 07:03:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240408070326-0001/0 is now RUNNING\n",
      "24/04/08 07:03:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240408070326-0001/1 is now RUNNING\n",
      "24/04/08 07:03:26 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "Setu: running `PageAnalysisStage` stage/component\n",
      "24/04/08 07:03:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/04/08 07:03:26 INFO SharedState: Warehouse path is 'file:/home/shanks/setu/examples/spark-warehouse'.\n",
      "24/04/08 07:03:27 WARN DataSource: All paths were ignored:\n",
      "  \n",
      "24/04/08 07:03:27 INFO InMemoryFileIndex: It took 15 ms to list leaf files for 0 paths.\n",
      "24/04/08 07:03:28 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "24/04/08 07:03:28 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/04/08 07:03:28 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "24/04/08 07:03:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/04/08 07:03:28 INFO DAGScheduler: Missing parents: List()\n",
      "24/04/08 07:03:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/04/08 07:03:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.1 KiB, free 3.4 GiB)\n",
      "24/04/08 07:03:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 3.4 GiB)\n",
      "24/04/08 07:03:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:40889 (size: 37.7 KiB, free: 3.4 GiB)\n",
      "24/04/08 07:03:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "24/04/08 07:03:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/04/08 07:03:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/04/08 07:03:29 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:41774) with ID 0,  ResourceProfileId 0\n",
      "24/04/08 07:03:29 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:41766) with ID 1,  ResourceProfileId 0\n",
      "24/04/08 07:03:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.108.238:45275 with 1663.2 MiB RAM, BlockManagerId(0, 172.31.108.238, 45275, None)\n",
      "24/04/08 07:03:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.108.238:42025 with 1663.2 MiB RAM, BlockManagerId(1, 172.31.108.238, 42025, None)\n",
      "24/04/08 07:04:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.31.108.238, executor 1, partition 0, PROCESS_LOCAL, 7776 bytes) \n",
      "24/04/08 07:04:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.31.108.238:42025 (size: 37.7 KiB, free: 1663.2 MiB)\n",
      "24/04/08 07:04:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 491 ms on 172.31.108.238 (executor 1) (1/1)\n",
      "24/04/08 07:04:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/04/08 07:04:11 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 43.005 s\n",
      "24/04/08 07:04:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/04/08 07:04:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/04/08 07:04:11 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 43.035826 s\n",
      "Encountered an Error:  [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shanks/setu/setu/run.py\", line 59, in <module>\n",
      "    setu.run(spark, stage, **args)\n",
      "  File \"/home/shanks/setu/setu/main.py\", line 184, in run\n",
      "    return self.run_component(spark=spark, component=component, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/main.py\", line 168, in run_component\n",
      "    return self.components[self._stage_component_mapping[component]].run(spark=spark, stage=component, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/ocr_processing.py\", line 435, in run\n",
      "    return self.run_stage(spark=spark, stage=stage, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/ocr_processing.py\", line 427, in run_stage\n",
      "    return self.stages[stage].run(spark, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/ocr_processing.py\", line 358, in run\n",
      "    return self.run_spark(spark, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/ocr_processing.py\", line 343, in run_spark\n",
      "    df = spark.read.format(\"parquet\").load(pa_parquets_path)\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 314, in load\n",
      "    return self._df(self._jreader.load())\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.\n",
      "24/04/08 07:04:11 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/04/08 07:04:11 INFO SparkUI: Stopped Spark web UI at http://localhost:4040\n",
      "24/04/08 07:04:11 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/04/08 07:04:11 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/04/08 07:04:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/04/08 07:04:11 INFO MemoryStore: MemoryStore cleared\n",
      "24/04/08 07:04:11 INFO BlockManager: BlockManager stopped\n",
      "24/04/08 07:04:11 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/04/08 07:04:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/04/08 07:04:11 INFO SparkContext: Successfully stopped SparkContext\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shanks/setu/setu/run.py\", line 59, in <module>\n",
      "    setu.run(spark, stage, **args)\n",
      "  File \"/home/shanks/setu/setu/main.py\", line 184, in run\n",
      "    return self.run_component(spark=spark, component=component, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/main.py\", line 168, in run_component\n",
      "    return self.components[self._stage_component_mapping[component]].run(spark=spark, stage=component, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/ocr_processing.py\", line 435, in run\n",
      "    return self.run_stage(spark=spark, stage=stage, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/ocr_processing.py\", line 427, in run_stage\n",
      "    return self.stages[stage].run(spark, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/ocr_processing.py\", line 358, in run\n",
      "    return self.run_spark(spark, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/ocr_processing.py\", line 343, in run_spark\n",
      "    df = spark.read.format(\"parquet\").load(pa_parquets_path)\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 314, in load\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shanks/setu/setu/run.py\", line 67, in <module>\n",
      "    raise Exception(\"Job Failed with above mentioned exception\")\n",
      "Exception: Job Failed with above mentioned exception\n",
      "24/04/08 07:04:12 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/04/08 07:04:12 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-ddb28217-6173-4c4e-bca6-b5fda9da9caa\n",
      "24/04/08 07:04:12 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-92ede60f-2247-4548-b1b5-57dafa9711d2\n",
      "24/04/08 07:04:12 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-1793a544-fc91-4c71-943d-df298e262bc4\n",
      "24/04/08 07:04:12 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-1793a544-fc91-4c71-943d-df298e262bc4/pyspark-a31f428d-6112-41d1-95f6-09d72f478282\n"
     ]
    }
   ],
   "source": [
    "!SETU_DIR=/home/$USER/setu SETU_TMP_DIR=/home/$USER/tmp/ FILTER_DATA_ROOT=/home/$USER/setu/setu/data \\\n",
    "    spark-submit \\\n",
    "    --master spark://YDEARYZEN.:7077 \\\n",
    "    --deploy-mode client \\\n",
    "    --driver-java-options -Djava.io.tmpdir=/home/$USER/tmp/ \\\n",
    "    --conf \"spark.driver.extraJavaOptions=-Djava.io.tmpdir=/home/$USER/tmp/\" \\\n",
    "    --conf \"spark.executor.extraJavaOptions=-Djava.io.tmpdir=/home/$USER/tmp/\" \\\n",
    "    --conf spark.worker.dir=\"/home/$USER/tmp/\" \\\n",
    "    --conf spark.local.dir=\"/home/$USER/tmp/\" \\\n",
    "    --num-executors 4 \\\n",
    "    --executor-cores 2 \\\n",
    "    --executor-memory 3G \\\n",
    "    --driver-memory 6G \\\n",
    "    --archives \"/home/$USER/setu/dataproc/envs/setu.zip\" \\\n",
    "    --conf 'spark.executorEnv.PYTHONPATH=setu.zip' \\\n",
    "    --conf 'spark.executorEnv.FILTER_DATA_ROOT=setu.zip/data' \\\n",
    "    /home/$USER/setu/setu/run.py \\\n",
    "    --config /home/$USER/setu/configs/ocr/spark_tamil_config.json \\\n",
    "    --mode ocr \\\n",
    "    --run_local True \\\n",
    "    PageAnalysisStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "setu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
