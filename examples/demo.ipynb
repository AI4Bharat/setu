{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your data folder to ensure availability of files to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_english_pdf.parquet', 'sample_english_crawl.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(f\"/home/{os.environ.get('USER')}/setu/examples/sample_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the JSON2Parquet Stage to convert your data jsons into parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/07 01:10:38 WARN Utils: Your hostname, YDEARYZEN resolves to a loopback address: 127.0.1.1; using 172.31.108.238 instead (on interface eth0)\n",
      "24/03/07 01:10:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/07 01:10:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Command used to run this script:  /home/shanks/setu/setu/run.py --config /home/shanks/setu/configs/crawls/spark_english_config.json --mode crawl --run_local True JSON2ParquetStage --json_glob_path /home/shanks/setu/examples/sample_data/*.json --language english --j2p_samples_per_partition 1500 --j2p_verbose False --j2p_run_mode data --j2p_parquet_output_path /home/shanks/setu/examples/output/j2p_output\n",
      "shanks\n",
      "------------------------------------------------ Setting Environment Variables --------------------------------------------------\n",
      "------------------------------------------------ END --------------------------------------------------\n",
      "------------------------------------------------ Environment Variables --------------------------------------------------\n",
      "PATH: /home/shanks/miniconda3/envs/setu_env/bin:/home/shanks/.vscode-server/bin/019f4d1419fbc8219a181fab7892ebccf7ee29a2/bin/remote-cli:/home/shanks/hadoop/spark-3.5.1/bin:/home/shanks/miniconda3/envs/setu_env/bin:/home/shanks/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files (x86)/VMware/VMware Player/bin:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/dotnet:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files/Go/bin:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/Program Files/RabbitMQ Server/rabbitmq_server-3.12.13/sbin:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311/Scripts:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311:/mnt/c/Users/Shanks/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Shanks/AppData/Roaming/npm:/mnt/c/Users/Shanks/AppData/Local/Google/Cloud SDK/google-cloud-sdk/bin:/mnt/c/Users/Shanks/miniconda3/Scripts:/mnt/c/Users/Shanks/miniconda3:/mnt/c/Users/Shanks/miniconda3/Library/bin/conda.bat:/snap/bin\n",
      "XDG_DATA_DIRS: /usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "CONDA_DEFAULT_ENV: setu_env\n",
      "GIT_PAGER: cat\n",
      "CLICOLOR_FORCE: 1\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "SPARK_ENV_LOADED: 1\n",
      "CONDA_PYTHON_EXE: /home/shanks/miniconda3/bin/python\n",
      "MOTD_SHOWN: update-motd\n",
      "WSL2_GUI_APPS_ENABLED: 1\n",
      "DBUS_SESSION_BUS_ADDRESS: unix:path=/run/user/1000/bus\n",
      "FILTER_DATA_ROOT: /home/shanks/setu/setu/data\n",
      "CONDA_PREFIX: /home/shanks/miniconda3/envs/setu_env\n",
      "CONDA_PREFIX_1: /home/shanks/miniconda3\n",
      "LOGNAME: shanks\n",
      "SETU_TMP_DIR: /home/shanks/tmp/\n",
      "PWD: /home/shanks/setu/examples\n",
      "VSCODE_NLS_CONFIG: {\"locale\":\"en\",\"osLocale\":\"en\",\"availableLanguages\":{}}\n",
      "PYTHONPATH: /home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip:/home/shanks/hadoop/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip:/home/shanks/hadoop/spark-3.5.1/jars/spark-core_2.12-3.5.1.jar\n",
      "LESSOPEN: | /usr/bin/lesspipe %s\n",
      "SHELL: /bin/bash\n",
      "WSL_INTEROP: /run/WSL/379_interop\n",
      "CONDA_ROOT: /home/shanks/miniconda3\n",
      "PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "HOSTTYPE: x86_64\n",
      "VSCODE_WSL_EXT_LOCATION: /mnt/c/Users/Shanks/.vscode/extensions/ms-vscode-remote.remote-wsl-0.86.0\n",
      "SPARK_CONF_DIR: /home/shanks/hadoop/spark-3.5.1/conf\n",
      "NAME: YDEARYZEN\n",
      "CONDA_PROMPT_MODIFIER: (setu_env) \n",
      "LS_COLORS: rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\n",
      "SHLVL: 1\n",
      "GSETTINGS_SCHEMA_DIR_CONDA_BACKUP: \n",
      "SETU_DIR: /home/shanks/setu\n",
      "VSCODE_HANDLES_SIGPIPE: true\n",
      "LESSCLOSE: /usr/bin/lesspipe %s %s\n",
      "FORCE_COLOR: 1\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "CONDA_EXE: /home/shanks/miniconda3/bin/conda\n",
      "TERM: xterm-color\n",
      "LANG: C.UTF-8\n",
      "SPARK_SCALA_VERSION: 2.12\n",
      "DISPLAY: :0\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "SPARK_HOME: /home/shanks/hadoop/spark-3.5.1\n",
      "_CE_M: \n",
      "WAYLAND_DISPLAY: wayland-0\n",
      "VSCODE_IPC_HOOK_CLI: /run/user/1000/vscode-ipc-27ddf6b5-ef93-444d-97b6-5692a863b9a1.sock\n",
      "VSCODE_AMD_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "CONDA_SHLVL: 2\n",
      "PYTHONHASHSEED: 0\n",
      "VSCODE_CWD: /mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code\n",
      "WSL_DISTRO_NAME: Ubuntu\n",
      "PYTHONIOENCODING: utf-8\n",
      "PYSPARK_GATEWAY_SECRET: 766af3acc584d0f2847c06853ded7643229d947f9120eae614218340958f9672\n",
      "USER: shanks\n",
      "CLICOLOR: 1\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "PULSE_SERVER: unix:/mnt/wslg/PulseServer\n",
      "PYSPARK_GATEWAY_PORT: 40885\n",
      "_CE_CONDA: \n",
      "PYTHONUNBUFFERED: YES\n",
      "WSLENV: VSCODE_WSL_EXT_LOCATION/up\n",
      "GSETTINGS_SCHEMA_DIR: /home/shanks/miniconda3/envs/setu_env/share/glib-2.0/schemas\n",
      "XDG_RUNTIME_DIR: /run/user/1000/\n",
      "HOME: /home/shanks\n",
      "------------------------------------------------ END --------------------------------------------------\n",
      "------------------------------------------------ Directory Structure --------------------------------------------------\n",
      "commands.md\n",
      "demo.ipynb\n",
      "my_commands.md\n",
      "output\n",
      "sample_data\n",
      "ls: cannot access 'setu.zip': No such file or directory\n",
      "------------------------------------------------ End --------------------------------------------------\n",
      "Command used to run this script:  /home/shanks/setu/setu/run.py --config /home/shanks/setu/configs/crawls/spark_english_config.json --mode crawl --run_local True JSON2ParquetStage --json_glob_path /home/shanks/setu/examples/sample_data/*.json --language english --j2p_samples_per_partition 1500 --j2p_verbose False --j2p_run_mode data --j2p_parquet_output_path /home/shanks/setu/examples/output/j2p_output\n",
      "Entered Commandline Arguments:  Namespace(config='/home/shanks/setu/configs/crawls/spark_english_config.json', mode='crawl', run_local=True, stage='JSON2ParquetStage', json_glob_path='/home/shanks/setu/examples/sample_data/*.json', j2p_cols=None, language='english', j2p_samples_per_partition=1500, j2p_verbose=False, j2p_run_mode='data', j2p_is_multiline=False, j2p_parquet_output_path='/home/shanks/setu/examples/output/j2p_output', j2p_bucket=None, j2p_bucket_prefix=None)\n",
      "PySpark Installed. Going forward with spark execution for JSON2ParquetStage\n",
      "PySpark Installed. Going forward with spark execution for ExtractTextStage\n",
      "PySpark Installed. Going forward with spark execution for FormatConversionStage\n",
      "PySpark Installed. Going forward with spark execution for PageAnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for PageFilteringStage\n",
      "PySpark Installed. Going forward with spark execution for PageTextExtractionStage\n",
      "PySpark Installed. Going forward with spark execution for PageMergeStage\n",
      "PySpark Installed. Going forward with spark execution for DocCleanStage\n",
      "PySpark Installed. Going forward with spark execution for LIDStage\n",
      "PySpark Installed. Going forward with spark execution for AnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for FlaggingAndFilteringStage\n",
      "PySpark Installed. Going forward with spark execution for DocumentRemovalStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeJson2ParquetStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeDocCleanStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeLIDStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeAnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeFilterStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeMinhashStage\n",
      "PySpark Installed. Going forward with spark execution for CustomJoinStage\n",
      "PySpark Installed. Going forward with spark execution for Page2PDFStage\n",
      "PySpark Installed. Going forward with spark execution for NormalizeStage\n",
      "24/03/07 01:10:43 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/03/07 01:10:43 INFO SparkContext: OS info Linux, 5.15.133.1-microsoft-standard-WSL2, amd64\n",
      "24/03/07 01:10:43 INFO SparkContext: Java version 11.0.22\n",
      "24/03/07 01:10:43 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/03/07 01:10:43 INFO ResourceUtils: ==============================================================\n",
      "24/03/07 01:10:43 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/03/07 01:10:43 INFO ResourceUtils: ==============================================================\n",
      "24/03/07 01:10:43 INFO SparkContext: Submitted application: Sangraha Anaylsis\n",
      "24/03/07 01:10:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/03/07 01:10:43 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "24/03/07 01:10:43 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/03/07 01:10:43 INFO SecurityManager: Changing view acls to: shanks\n",
      "24/03/07 01:10:43 INFO SecurityManager: Changing modify acls to: shanks\n",
      "24/03/07 01:10:43 INFO SecurityManager: Changing view acls groups to: \n",
      "24/03/07 01:10:43 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/03/07 01:10:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: shanks; groups with view permissions: EMPTY; users with modify permissions: shanks; groups with modify permissions: EMPTY\n",
      "24/03/07 01:10:43 INFO Utils: Successfully started service 'sparkDriver' on port 35245.\n",
      "24/03/07 01:10:43 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/03/07 01:10:43 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/03/07 01:10:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/03/07 01:10:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/03/07 01:10:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/03/07 01:10:43 INFO DiskBlockManager: Created local directory at /home/shanks/tmp/blockmgr-3460269f-70c1-453b-9dcb-67e1808cf5c2\n",
      "24/03/07 01:10:43 INFO MemoryStore: MemoryStore started with capacity 3.4 GiB\n",
      "24/03/07 01:10:43 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/03/07 01:10:43 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/03/07 01:10:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/03/07 01:10:43 INFO SparkContext: Added archive file:///home/shanks/setu/dataproc/envs/setu.zip at spark://localhost:35245/files/setu.zip with timestamp 1709754043149\n",
      "24/03/07 01:10:43 INFO Utils: Copying /home/shanks/setu/dataproc/envs/setu.zip to /home/shanks/tmp/spark-011606fc-3ffe-4b21-9c5e-9bde8cddb7fa/setu.zip\n",
      "24/03/07 01:10:45 INFO SparkContext: Unpacking an archive file:///home/shanks/setu/dataproc/envs/setu.zip from /home/shanks/tmp/spark-011606fc-3ffe-4b21-9c5e-9bde8cddb7fa/setu.zip to /home/shanks/tmp/spark-48d45d0e-6e4f-40e5-b196-9db4e068dd70/userFiles-89d65bbc-5a1c-404e-87b9-2dcabdd65e43/setu.zip\n",
      "24/03/07 01:10:58 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://YDEARYZEN.:7077...\n",
      "24/03/07 01:10:58 INFO TransportClientFactory: Successfully created connection to YDEARYZEN./127.0.1.1:7077 after 24 ms (0 ms spent in bootstraps)\n",
      "24/03/07 01:10:58 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240307011058-0000\n",
      "24/03/07 01:10:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41291.\n",
      "24/03/07 01:10:58 INFO NettyBlockTransferService: Server created on localhost:41291\n",
      "24/03/07 01:10:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/03/07 01:10:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240307011058-0000/0 on worker-20240307011034-172.31.108.238-44751 (172.31.108.238:44751) with 2 core(s)\n",
      "24/03/07 01:10:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20240307011058-0000/0 on hostPort 172.31.108.238:44751 with 2 core(s), 3.0 GiB RAM\n",
      "24/03/07 01:10:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 41291, None)\n",
      "24/03/07 01:10:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240307011058-0000/1 on worker-20240307011034-172.31.108.238-44751 (172.31.108.238:44751) with 2 core(s)\n",
      "24/03/07 01:10:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20240307011058-0000/1 on hostPort 172.31.108.238:44751 with 2 core(s), 3.0 GiB RAM\n",
      "24/03/07 01:10:58 INFO BlockManagerMasterEndpoint: Registering block manager localhost:41291 with 3.4 GiB RAM, BlockManagerId(driver, localhost, 41291, None)\n",
      "24/03/07 01:10:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 41291, None)\n",
      "24/03/07 01:10:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 41291, None)\n",
      "24/03/07 01:10:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240307011058-0000/0 is now RUNNING\n",
      "24/03/07 01:10:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240307011058-0000/1 is now RUNNING\n",
      "24/03/07 01:10:58 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "Setu: running `JSON2ParquetStage` stage/component\n",
      "TOTAL PAGE COUNT to Process:  1\n",
      "24/03/07 01:10:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/03/07 01:10:59 INFO SharedState: Warehouse path is 'file:/home/shanks/setu/examples/spark-warehouse'.\n",
      "24/03/07 01:11:01 INFO CodeGenerator: Code generated in 265.505481 ms\n",
      "24/03/07 01:11:01 INFO DAGScheduler: Registering RDD 6 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/03/07 01:11:01 INFO DAGScheduler: Got map stage job 0 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/03/07 01:11:01 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:11:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/07 01:11:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:11:01 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:11:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 15.8 KiB, free 3.4 GiB)\n",
      "24/03/07 01:11:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 3.4 GiB)\n",
      "24/03/07 01:11:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:41291 (size: 8.3 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:11:01 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:11:01 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/07 01:11:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/03/07 01:11:01 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:53244) with ID 0,  ResourceProfileId 0\n",
      "24/03/07 01:11:01 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:53256) with ID 1,  ResourceProfileId 0\n",
      "24/03/07 01:11:01 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.108.238:45953 with 1663.2 MiB RAM, BlockManagerId(1, 172.31.108.238, 45953, None)\n",
      "24/03/07 01:11:01 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.108.238:41167 with 1663.2 MiB RAM, BlockManagerId(0, 172.31.108.238, 41167, None)\n",
      "24/03/07 01:11:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.31.108.238, executor 0, partition 0, PROCESS_LOCAL, 7753 bytes) \n",
      "24/03/07 01:11:57 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.31.108.238, executor 0, partition 1, PROCESS_LOCAL, 7847 bytes) \n",
      "24/03/07 01:11:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.31.108.238:41167 (size: 8.3 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:11:58 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1379 ms on 172.31.108.238 (executor 0) (1/2)\n",
      "24/03/07 01:11:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1397 ms on 172.31.108.238 (executor 0) (2/2)\n",
      "24/03/07 01:11:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:11:58 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 46281\n",
      "24/03/07 01:11:58 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 57.043 s\n",
      "24/03/07 01:11:58 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/07 01:11:58 INFO DAGScheduler: running: Set()\n",
      "24/03/07 01:11:58 INFO DAGScheduler: waiting: Set()\n",
      "24/03/07 01:11:58 INFO DAGScheduler: failed: Set()\n",
      "24/03/07 01:11:58 INFO CodeGenerator: Code generated in 11.153929 ms\n",
      "24/03/07 01:11:58 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Got job 1 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Final stage: ResultStage 2 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:11:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.5 KiB, free 3.4 GiB)\n",
      "24/03/07 01:11:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 3.4 GiB)\n",
      "24/03/07 01:11:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:41291 (size: 5.9 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:11:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:11:58 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:11:58 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.31.108.238, executor 0, partition 0, NODE_LOCAL, 7784 bytes) \n",
      "24/03/07 01:11:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.31.108.238:41167 (size: 5.9 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:11:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 127.0.0.1:53244\n",
      "24/03/07 01:11:58 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 179 ms on 172.31.108.238 (executor 0) (1/1)\n",
      "24/03/07 01:11:58 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:11:58 INFO DAGScheduler: ResultStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.189 s\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/07 01:11:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Job 1 finished: count at NativeMethodAccessorImpl.java:0, took 0.201666 s\n",
      "When required data will be repartitioned into - 1 partitions\n",
      "24/03/07 01:11:58 INFO CodeGenerator: Code generated in 12.309328 ms\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Registering RDD 11 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Got map stage job 2 (javaToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:11:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.6 KiB, free 3.4 GiB)\n",
      "24/03/07 01:11:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.2 KiB, free 3.4 GiB)\n",
      "24/03/07 01:11:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:41291 (size: 8.2 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:11:58 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:11:58 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/07 01:11:58 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
      "24/03/07 01:11:58 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.31.108.238, executor 0, partition 0, PROCESS_LOCAL, 7753 bytes) \n",
      "24/03/07 01:11:58 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (172.31.108.238, executor 1, partition 1, PROCESS_LOCAL, 7847 bytes) \n",
      "24/03/07 01:11:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:41291 in memory (size: 5.9 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:11:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.31.108.238:41167 (size: 8.2 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:11:58 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.31.108.238:41167 in memory (size: 5.9 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:11:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:41291 in memory (size: 8.3 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:11:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.31.108.238:41167 in memory (size: 8.3 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:11:59 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 141 ms on 172.31.108.238 (executor 0) (1/2)\n",
      "24/03/07 01:11:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.31.108.238:45953 (size: 8.2 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:12:00 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 1103 ms on 172.31.108.238 (executor 1) (2/2)\n",
      "24/03/07 01:12:00 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:12:00 INFO DAGScheduler: ShuffleMapStage 3 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 1.119 s\n",
      "24/03/07 01:12:00 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/07 01:12:00 INFO DAGScheduler: running: Set()\n",
      "24/03/07 01:12:00 INFO DAGScheduler: waiting: Set()\n",
      "24/03/07 01:12:00 INFO DAGScheduler: failed: Set()\n",
      "24/03/07 01:12:00 INFO CodeGenerator: Code generated in 5.97893 ms\n",
      "24/03/07 01:12:00 INFO CodeGenerator: Code generated in 13.196158 ms\n",
      "24/03/07 01:12:00 INFO DAGScheduler: Registering RDD 22 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "24/03/07 01:12:00 INFO DAGScheduler: Got map stage job 3 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:12:00 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:12:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "24/03/07 01:12:00 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:12:00 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[22] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:12:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 34.0 KiB, free 3.4 GiB)\n",
      "24/03/07 01:12:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 15.8 KiB, free 3.4 GiB)\n",
      "24/03/07 01:12:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:41291 (size: 15.8 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:12:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:12:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:41291 in memory (size: 8.2 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:12:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[22] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:12:00 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:12:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.31.108.238:41167 in memory (size: 8.2 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:12:00 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.31.108.238, executor 0, partition 0, NODE_LOCAL, 7773 bytes) \n",
      "24/03/07 01:12:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.31.108.238:45953 in memory (size: 8.2 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:12:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.31.108.238:41167 (size: 15.8 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:12:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 127.0.0.1:53244\n",
      "24/03/07 01:12:03 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 3544 ms on 172.31.108.238 (executor 0) (1/1)\n",
      "24/03/07 01:12:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:12:03 INFO DAGScheduler: ShuffleMapStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 3.561 s\n",
      "24/03/07 01:12:03 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/07 01:12:03 INFO DAGScheduler: running: Set()\n",
      "24/03/07 01:12:03 INFO DAGScheduler: waiting: Set()\n",
      "24/03/07 01:12:03 INFO DAGScheduler: failed: Set()\n",
      "24/03/07 01:12:03 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/03/07 01:12:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/03/07 01:12:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/03/07 01:12:03 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/03/07 01:12:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/03/07 01:12:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/03/07 01:12:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/03/07 01:12:03 INFO CodeGenerator: Code generated in 11.383881 ms\n",
      "24/03/07 01:12:03 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "24/03/07 01:12:03 INFO DAGScheduler: Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:12:03 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:12:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "24/03/07 01:12:03 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:12:03 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[25] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:12:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 216.4 KiB, free 3.4 GiB)\n",
      "24/03/07 01:12:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 77.7 KiB, free 3.4 GiB)\n",
      "24/03/07 01:12:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:41291 (size: 77.7 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:12:03 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:12:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:41291 in memory (size: 15.8 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:12:03 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.31.108.238:41167 in memory (size: 15.8 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:12:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:12:03 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:12:03 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (172.31.108.238, executor 1, partition 0, NODE_LOCAL, 7784 bytes) \n",
      "24/03/07 01:12:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.31.108.238:45953 (size: 77.7 KiB, free: 1663.1 MiB)\n",
      "24/03/07 01:12:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 127.0.0.1:53256\n",
      "24/03/07 01:12:04 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 851 ms on 172.31.108.238 (executor 1) (1/1)\n",
      "24/03/07 01:12:04 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:12:04 INFO DAGScheduler: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.896 s\n",
      "24/03/07 01:12:04 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/07 01:12:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/03/07 01:12:04 INFO DAGScheduler: Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.902034 s\n",
      "24/03/07 01:12:04 INFO FileFormatWriter: Start to commit write Job e5008d4b-a4ff-4a4e-8295-818786776df3.\n",
      "24/03/07 01:12:04 INFO FileFormatWriter: Write Job e5008d4b-a4ff-4a4e-8295-818786776df3 committed. Elapsed time: 17 ms.\n",
      "24/03/07 01:12:04 INFO FileFormatWriter: Finished processing stats for write job e5008d4b-a4ff-4a4e-8295-818786776df3.\n",
      "24/03/07 01:12:05 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "24/03/07 01:12:05 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/03/07 01:12:05 INFO SparkUI: Stopped Spark web UI at http://localhost:4040\n",
      "24/03/07 01:12:05 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/03/07 01:12:05 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/03/07 01:12:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/03/07 01:12:05 INFO MemoryStore: MemoryStore cleared\n",
      "24/03/07 01:12:05 INFO BlockManager: BlockManager stopped\n",
      "24/03/07 01:12:05 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/03/07 01:12:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/03/07 01:12:05 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/03/07 01:12:05 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/03/07 01:12:05 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-011606fc-3ffe-4b21-9c5e-9bde8cddb7fa\n",
      "24/03/07 01:12:05 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-48d45d0e-6e4f-40e5-b196-9db4e068dd70\n",
      "24/03/07 01:12:05 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-672a289d-306f-4082-a1d2-dc6b4d44156d\n",
      "24/03/07 01:12:05 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-48d45d0e-6e4f-40e5-b196-9db4e068dd70/pyspark-83074ec2-c9d1-4cda-b540-3fe7b79bc830\n"
     ]
    }
   ],
   "source": [
    "!SETU_DIR=/home/$USER/setu SETU_TMP_DIR=/home/$USER/tmp/ FILTER_DATA_ROOT=/home/$USER/setu/setu/data \\\n",
    "    spark-submit \\\n",
    "    --master spark://YDEARYZEN.:7077 \\\n",
    "    --deploy-mode client \\\n",
    "    --driver-java-options -Djava.io.tmpdir=/home/$USER/tmp/ \\\n",
    "    --conf \"spark.driver.extraJavaOptions=-Djava.io.tmpdir=/home/$USER/tmp/\" \\\n",
    "    --conf \"spark.executor.extraJavaOptions=-Djava.io.tmpdir=/home/$USER/tmp/\" \\\n",
    "    --conf spark.worker.dir=\"/home/$USER/tmp/\" \\\n",
    "    --conf spark.local.dir=\"/home/$USER/tmp/\" \\\n",
    "    --num-executors 4 \\\n",
    "    --executor-cores 2 \\\n",
    "    --executor-memory 3G \\\n",
    "    --driver-memory 6G \\\n",
    "    --archives \"/home/$USER/setu/dataproc/envs/setu.zip\" \\\n",
    "    --conf 'spark.executorEnv.PYTHONPATH=setu.zip' \\\n",
    "    --conf 'spark.executorEnv.FILTER_DATA_ROOT=setu.zip/data' \\\n",
    "    /home/$USER/setu/setu/run.py \\\n",
    "    --config /home/$USER/setu/configs/crawls/spark_english_config.json \\\n",
    "    --mode crawl \\\n",
    "    --run_local True \\\n",
    "    JSON2ParquetStage \\\n",
    "    --json_glob_path \"/home/$USER/setu/examples/sample_data/*.json\" \\\n",
    "    --language english \\\n",
    "    --j2p_samples_per_partition 1500 \\\n",
    "    --j2p_verbose False \\\n",
    "    --j2p_run_mode data \\\n",
    "    --j2p_parquet_output_path /home/$USER/setu/examples/output/j2p_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the JSON2Parquet Stage Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output_path = f\"/home/{os.environ.get('USER')}/setu/examples/output/j2p_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquets = glob(f\"{json_output_path}/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(parquets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8bc919b9-c288-42fd-b503-d0f366e06a14</td>\n",
       "      <td>https://www.91mobiles.com/nokia-t10-lte-price-...</td>\n",
       "      <td>91mobiles</td>\n",
       "      <td>english</td>\n",
       "      <td>&lt;html lang=\"en\"&gt;\\n&lt;head&gt;\\n        \\n        &lt;m...</td>\n",
       "      <td>10/07/23 10:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 doc_id  \\\n",
       "0  8bc919b9-c288-42fd-b503-d0f366e06a14   \n",
       "\n",
       "                                                 url     source language  \\\n",
       "0  https://www.91mobiles.com/nokia-t10-lte-price-...  91mobiles  english   \n",
       "\n",
       "                                                text       timestamp  \n",
       "0  <html lang=\"en\">\\n<head>\\n        \\n        <m...  10/07/23 10:54  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Text Extraction State to extract text from the previous stage output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/07 01:13:07 WARN Utils: Your hostname, YDEARYZEN resolves to a loopback address: 127.0.1.1; using 172.31.108.238 instead (on interface eth0)\n",
      "24/03/07 01:13:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/07 01:13:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Command used to run this script:  /home/shanks/setu/setu/run.py --config /home/shanks/setu/configs/crawls/spark_english_config.json --mode crawl --run_local True ExtractTextStage --te_parquets_path /home/shanks/setu/examples/output/j2p_output/*.parquet --te_samples_per_partition 1500 --te_run_mode data --te_output_path /home/shanks/setu/examples/output/te_output\n",
      "shanks\n",
      "------------------------------------------------ Setting Environment Variables --------------------------------------------------\n",
      "------------------------------------------------ END --------------------------------------------------\n",
      "------------------------------------------------ Environment Variables --------------------------------------------------\n",
      "PATH: /home/shanks/miniconda3/envs/setu_env/bin:/home/shanks/.vscode-server/bin/019f4d1419fbc8219a181fab7892ebccf7ee29a2/bin/remote-cli:/home/shanks/hadoop/spark-3.5.1/bin:/home/shanks/miniconda3/envs/setu_env/bin:/home/shanks/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files (x86)/VMware/VMware Player/bin:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/dotnet:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files/Go/bin:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/Program Files/RabbitMQ Server/rabbitmq_server-3.12.13/sbin:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311/Scripts:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311:/mnt/c/Users/Shanks/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Shanks/AppData/Roaming/npm:/mnt/c/Users/Shanks/AppData/Local/Google/Cloud SDK/google-cloud-sdk/bin:/mnt/c/Users/Shanks/miniconda3/Scripts:/mnt/c/Users/Shanks/miniconda3:/mnt/c/Users/Shanks/miniconda3/Library/bin/conda.bat:/snap/bin\n",
      "XDG_DATA_DIRS: /usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "CONDA_DEFAULT_ENV: setu_env\n",
      "GIT_PAGER: cat\n",
      "CLICOLOR_FORCE: 1\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "SPARK_ENV_LOADED: 1\n",
      "CONDA_PYTHON_EXE: /home/shanks/miniconda3/bin/python\n",
      "MOTD_SHOWN: update-motd\n",
      "WSL2_GUI_APPS_ENABLED: 1\n",
      "DBUS_SESSION_BUS_ADDRESS: unix:path=/run/user/1000/bus\n",
      "FILTER_DATA_ROOT: /home/shanks/setu/setu/data\n",
      "CONDA_PREFIX: /home/shanks/miniconda3/envs/setu_env\n",
      "CONDA_PREFIX_1: /home/shanks/miniconda3\n",
      "LOGNAME: shanks\n",
      "SETU_TMP_DIR: /home/shanks/tmp/\n",
      "PWD: /home/shanks/setu/examples\n",
      "VSCODE_NLS_CONFIG: {\"locale\":\"en\",\"osLocale\":\"en\",\"availableLanguages\":{}}\n",
      "PYTHONPATH: /home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip:/home/shanks/hadoop/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip:/home/shanks/hadoop/spark-3.5.1/jars/spark-core_2.12-3.5.1.jar\n",
      "LESSOPEN: | /usr/bin/lesspipe %s\n",
      "SHELL: /bin/bash\n",
      "WSL_INTEROP: /run/WSL/379_interop\n",
      "CONDA_ROOT: /home/shanks/miniconda3\n",
      "PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "HOSTTYPE: x86_64\n",
      "VSCODE_WSL_EXT_LOCATION: /mnt/c/Users/Shanks/.vscode/extensions/ms-vscode-remote.remote-wsl-0.86.0\n",
      "SPARK_CONF_DIR: /home/shanks/hadoop/spark-3.5.1/conf\n",
      "NAME: YDEARYZEN\n",
      "CONDA_PROMPT_MODIFIER: (setu_env) \n",
      "LS_COLORS: rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\n",
      "SHLVL: 1\n",
      "GSETTINGS_SCHEMA_DIR_CONDA_BACKUP: \n",
      "SETU_DIR: /home/shanks/setu\n",
      "VSCODE_HANDLES_SIGPIPE: true\n",
      "LESSCLOSE: /usr/bin/lesspipe %s %s\n",
      "FORCE_COLOR: 1\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "CONDA_EXE: /home/shanks/miniconda3/bin/conda\n",
      "TERM: xterm-color\n",
      "LANG: C.UTF-8\n",
      "SPARK_SCALA_VERSION: 2.12\n",
      "DISPLAY: :0\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "SPARK_HOME: /home/shanks/hadoop/spark-3.5.1\n",
      "_CE_M: \n",
      "WAYLAND_DISPLAY: wayland-0\n",
      "VSCODE_IPC_HOOK_CLI: /run/user/1000/vscode-ipc-27ddf6b5-ef93-444d-97b6-5692a863b9a1.sock\n",
      "VSCODE_AMD_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "CONDA_SHLVL: 2\n",
      "PYTHONHASHSEED: 0\n",
      "VSCODE_CWD: /mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code\n",
      "WSL_DISTRO_NAME: Ubuntu\n",
      "PYTHONIOENCODING: utf-8\n",
      "PYSPARK_GATEWAY_SECRET: 4faf19e6745f02e886ce0eb79fb8a7b5939151d77f2e73c7a837cac5535feaa6\n",
      "USER: shanks\n",
      "CLICOLOR: 1\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "PULSE_SERVER: unix:/mnt/wslg/PulseServer\n",
      "PYSPARK_GATEWAY_PORT: 46365\n",
      "_CE_CONDA: \n",
      "PYTHONUNBUFFERED: YES\n",
      "WSLENV: VSCODE_WSL_EXT_LOCATION/up\n",
      "GSETTINGS_SCHEMA_DIR: /home/shanks/miniconda3/envs/setu_env/share/glib-2.0/schemas\n",
      "XDG_RUNTIME_DIR: /run/user/1000/\n",
      "HOME: /home/shanks\n",
      "------------------------------------------------ END --------------------------------------------------\n",
      "------------------------------------------------ Directory Structure --------------------------------------------------\n",
      "commands.md\n",
      "demo.ipynb\n",
      "my_commands.md\n",
      "output\n",
      "sample_data\n",
      "ls: cannot access 'setu.zip': No such file or directory\n",
      "------------------------------------------------ End --------------------------------------------------\n",
      "Command used to run this script:  /home/shanks/setu/setu/run.py --config /home/shanks/setu/configs/crawls/spark_english_config.json --mode crawl --run_local True ExtractTextStage --te_parquets_path /home/shanks/setu/examples/output/j2p_output/*.parquet --te_samples_per_partition 1500 --te_run_mode data --te_output_path /home/shanks/setu/examples/output/te_output\n",
      "Entered Commandline Arguments:  Namespace(config='/home/shanks/setu/configs/crawls/spark_english_config.json', mode='crawl', run_local=True, stage='ExtractTextStage', te_parquets_path='/home/shanks/setu/examples/output/j2p_output/*.parquet', te_samples_per_partition=1500, te_verbose=False, te_run_mode='data', te_output_path='/home/shanks/setu/examples/output/te_output')\n",
      "PySpark Installed. Going forward with spark execution for JSON2ParquetStage\n",
      "PySpark Installed. Going forward with spark execution for ExtractTextStage\n",
      "PySpark Installed. Going forward with spark execution for FormatConversionStage\n",
      "PySpark Installed. Going forward with spark execution for PageAnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for PageFilteringStage\n",
      "PySpark Installed. Going forward with spark execution for PageTextExtractionStage\n",
      "PySpark Installed. Going forward with spark execution for PageMergeStage\n",
      "PySpark Installed. Going forward with spark execution for DocCleanStage\n",
      "PySpark Installed. Going forward with spark execution for LIDStage\n",
      "PySpark Installed. Going forward with spark execution for AnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for FlaggingAndFilteringStage\n",
      "PySpark Installed. Going forward with spark execution for DocumentRemovalStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeJson2ParquetStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeDocCleanStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeLIDStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeAnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeFilterStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeMinhashStage\n",
      "PySpark Installed. Going forward with spark execution for CustomJoinStage\n",
      "PySpark Installed. Going forward with spark execution for Page2PDFStage\n",
      "PySpark Installed. Going forward with spark execution for NormalizeStage\n",
      "24/03/07 01:13:11 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/03/07 01:13:11 INFO SparkContext: OS info Linux, 5.15.133.1-microsoft-standard-WSL2, amd64\n",
      "24/03/07 01:13:11 INFO SparkContext: Java version 11.0.22\n",
      "24/03/07 01:13:11 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/03/07 01:13:11 INFO ResourceUtils: ==============================================================\n",
      "24/03/07 01:13:11 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/03/07 01:13:11 INFO ResourceUtils: ==============================================================\n",
      "24/03/07 01:13:11 INFO SparkContext: Submitted application: Sangraha Anaylsis\n",
      "24/03/07 01:13:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/03/07 01:13:11 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "24/03/07 01:13:11 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/03/07 01:13:11 INFO SecurityManager: Changing view acls to: shanks\n",
      "24/03/07 01:13:11 INFO SecurityManager: Changing modify acls to: shanks\n",
      "24/03/07 01:13:11 INFO SecurityManager: Changing view acls groups to: \n",
      "24/03/07 01:13:11 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/03/07 01:13:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: shanks; groups with view permissions: EMPTY; users with modify permissions: shanks; groups with modify permissions: EMPTY\n",
      "24/03/07 01:13:12 INFO Utils: Successfully started service 'sparkDriver' on port 44883.\n",
      "24/03/07 01:13:12 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/03/07 01:13:12 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/03/07 01:13:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/03/07 01:13:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/03/07 01:13:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/03/07 01:13:12 INFO DiskBlockManager: Created local directory at /home/shanks/tmp/blockmgr-4f28cfc9-549d-429a-920a-25d38d2ba926\n",
      "24/03/07 01:13:12 INFO MemoryStore: MemoryStore started with capacity 3.4 GiB\n",
      "24/03/07 01:13:12 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/03/07 01:13:12 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/03/07 01:13:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/03/07 01:13:12 INFO SparkContext: Added archive file:///home/shanks/setu/dataproc/envs/setu.zip at spark://localhost:44883/files/setu.zip with timestamp 1709754191797\n",
      "24/03/07 01:13:12 INFO Utils: Copying /home/shanks/setu/dataproc/envs/setu.zip to /home/shanks/tmp/spark-b0dd0425-042d-48ab-98c0-881a27616ff2/setu.zip\n",
      "24/03/07 01:13:17 INFO SparkContext: Unpacking an archive file:///home/shanks/setu/dataproc/envs/setu.zip from /home/shanks/tmp/spark-b0dd0425-042d-48ab-98c0-881a27616ff2/setu.zip to /home/shanks/tmp/spark-2e0811aa-9eff-4ebc-ae63-5f0ef175b47b/userFiles-70a5c84d-1baa-4b8a-b841-99e69f829560/setu.zip\n",
      "24/03/07 01:13:31 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://YDEARYZEN.:7077...\n",
      "24/03/07 01:13:31 INFO TransportClientFactory: Successfully created connection to YDEARYZEN./127.0.1.1:7077 after 27 ms (0 ms spent in bootstraps)\n",
      "24/03/07 01:13:31 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240307011331-0001\n",
      "24/03/07 01:13:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240307011331-0001/0 on worker-20240307011034-172.31.108.238-44751 (172.31.108.238:44751) with 2 core(s)\n",
      "24/03/07 01:13:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20240307011331-0001/0 on hostPort 172.31.108.238:44751 with 2 core(s), 3.0 GiB RAM\n",
      "24/03/07 01:13:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240307011331-0001/1 on worker-20240307011034-172.31.108.238-44751 (172.31.108.238:44751) with 2 core(s)\n",
      "24/03/07 01:13:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20240307011331-0001/1 on hostPort 172.31.108.238:44751 with 2 core(s), 3.0 GiB RAM\n",
      "24/03/07 01:13:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43613.\n",
      "24/03/07 01:13:31 INFO NettyBlockTransferService: Server created on localhost:43613\n",
      "24/03/07 01:13:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/03/07 01:13:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 43613, None)\n",
      "24/03/07 01:13:31 INFO BlockManagerMasterEndpoint: Registering block manager localhost:43613 with 3.4 GiB RAM, BlockManagerId(driver, localhost, 43613, None)\n",
      "24/03/07 01:13:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 43613, None)\n",
      "24/03/07 01:13:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 43613, None)\n",
      "24/03/07 01:13:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240307011331-0001/0 is now RUNNING\n",
      "24/03/07 01:13:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240307011331-0001/1 is now RUNNING\n",
      "24/03/07 01:13:31 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "Setu: running `ExtractTextStage` stage/component\n",
      "24/03/07 01:13:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/03/07 01:13:31 INFO SharedState: Warehouse path is 'file:/home/shanks/setu/examples/spark-warehouse'.\n",
      "24/03/07 01:13:32 INFO InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.\n",
      "24/03/07 01:13:33 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "24/03/07 01:13:33 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:13:33 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:13:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/07 01:13:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:13:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:13:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.1 KiB, free 3.4 GiB)\n",
      "24/03/07 01:13:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 3.4 GiB)\n",
      "24/03/07 01:13:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:43613 (size: 37.7 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:13:33 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:13:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:13:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:13:34 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:57520) with ID 0,  ResourceProfileId 0\n",
      "24/03/07 01:13:34 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:57526) with ID 1,  ResourceProfileId 0\n",
      "24/03/07 01:13:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.108.238:37061 with 1663.2 MiB RAM, BlockManagerId(0, 172.31.108.238, 37061, None)\n",
      "24/03/07 01:13:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.108.238:38329 with 1663.2 MiB RAM, BlockManagerId(1, 172.31.108.238, 38329, None)\n",
      "24/03/07 01:14:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.31.108.238, executor 0, partition 0, PROCESS_LOCAL, 7959 bytes) \n",
      "24/03/07 01:14:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.31.108.238:37061 (size: 37.7 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 901 ms on 172.31.108.238 (executor 0) (1/1)\n",
      "24/03/07 01:14:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:14:21 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 48.416 s\n",
      "24/03/07 01:14:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/07 01:14:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/03/07 01:14:21 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 48.447600 s\n",
      "24/03/07 01:14:22 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:43613 in memory (size: 37.7 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:22 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.31.108.238:37061 in memory (size: 37.7 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:22 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/07 01:14:22 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/03/07 01:14:22 INFO CodeGenerator: Code generated in 155.672973 ms\n",
      "24/03/07 01:14:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 202.5 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:43613 (size: 35.3 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:22 INFO SparkContext: Created broadcast 1 from count at NativeMethodAccessorImpl.java:0\n",
      "24/03/07 01:14:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/07 01:14:22 INFO DAGScheduler: Registering RDD 6 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/03/07 01:14:22 INFO DAGScheduler: Got map stage job 1 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:14:22 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:14:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/07 01:14:22 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:14:23 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:14:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 31.0 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:43613 (size: 13.7 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:14:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:14:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:14:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.31.108.238, executor 1, partition 0, PROCESS_LOCAL, 8455 bytes) \n",
      "24/03/07 01:14:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.31.108.238:38329 (size: 13.7 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.31.108.238:38329 (size: 35.3 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1600 ms on 172.31.108.238 (executor 1) (1/1)\n",
      "24/03/07 01:14:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:14:24 INFO DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 1.659 s\n",
      "24/03/07 01:14:24 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/07 01:14:24 INFO DAGScheduler: running: Set()\n",
      "24/03/07 01:14:24 INFO DAGScheduler: waiting: Set()\n",
      "24/03/07 01:14:24 INFO DAGScheduler: failed: Set()\n",
      "24/03/07 01:14:24 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/03/07 01:14:24 INFO CodeGenerator: Code generated in 12.065937 ms\n",
      "24/03/07 01:14:24 INFO CodeGenerator: Code generated in 9.266061 ms\n",
      "24/03/07 01:14:24 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:43613 in memory (size: 13.7 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:24 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.31.108.238:38329 in memory (size: 13.7 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:24 INFO DAGScheduler: Registering RDD 11 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "24/03/07 01:14:24 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:14:24 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:14:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "24/03/07 01:14:24 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:14:24 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:14:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 38.0 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:43613 (size: 17.8 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:14:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:14:24 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:14:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.31.108.238, executor 0, partition 0, NODE_LOCAL, 7773 bytes) \n",
      "24/03/07 01:14:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.31.108.238:37061 (size: 17.8 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 127.0.0.1:57520\n",
      "24/03/07 01:14:25 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 970 ms on 172.31.108.238 (executor 0) (1/1)\n",
      "24/03/07 01:14:25 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:14:25 INFO DAGScheduler: ShuffleMapStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.983 s\n",
      "24/03/07 01:14:25 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/07 01:14:25 INFO DAGScheduler: running: Set()\n",
      "24/03/07 01:14:25 INFO DAGScheduler: waiting: Set()\n",
      "24/03/07 01:14:25 INFO DAGScheduler: failed: Set()\n",
      "24/03/07 01:14:25 INFO CodeGenerator: Code generated in 8.534719 ms\n",
      "24/03/07 01:14:25 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/03/07 01:14:25 INFO DAGScheduler: Got job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:14:25 INFO DAGScheduler: Final stage: ResultStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:14:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "24/03/07 01:14:25 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:14:25 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[14] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:14:25 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:25 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:43613 (size: 5.9 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:25 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:43613 in memory (size: 17.8 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:25 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:14:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[14] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:14:25 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:14:25 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.31.108.238:37061 in memory (size: 17.8 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:25 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (172.31.108.238, executor 0, partition 0, NODE_LOCAL, 7784 bytes) \n",
      "24/03/07 01:14:25 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.31.108.238:37061 (size: 5.9 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:25 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 127.0.0.1:57520\n",
      "24/03/07 01:14:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 81 ms on 172.31.108.238 (executor 0) (1/1)\n",
      "24/03/07 01:14:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:14:25 INFO DAGScheduler: ResultStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.093 s\n",
      "24/03/07 01:14:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/07 01:14:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/03/07 01:14:25 INFO DAGScheduler: Job 3 finished: count at NativeMethodAccessorImpl.java:0, took 0.096482 s\n",
      "When required data will be repartitioned into - 1 partitions\n",
      "24/03/07 01:14:26 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/07 01:14:26 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/03/07 01:14:26 INFO CodeGenerator: Code generated in 18.295601 ms\n",
      "24/03/07 01:14:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 203.0 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:43613 in memory (size: 5.9 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.31.108.238:37061 in memory (size: 5.9 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:43613 (size: 35.4 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO SparkContext: Created broadcast 5 from javaToPython at NativeMethodAccessorImpl.java:0\n",
      "24/03/07 01:14:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Registering RDD 19 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Got map stage job 4 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[19] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:14:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 36.0 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:43613 (size: 15.3 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[19] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:14:26 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:14:26 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 4) (172.31.108.238, executor 1, partition 0, PROCESS_LOCAL, 8455 bytes) \n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.31.108.238:38329 (size: 15.3 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.31.108.238:38329 (size: 35.4 KiB, free: 1663.1 MiB)\n",
      "24/03/07 01:14:26 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 4) in 249 ms on 172.31.108.238 (executor 1) (1/1)\n",
      "24/03/07 01:14:26 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:14:26 INFO DAGScheduler: ShuffleMapStage 7 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.276 s\n",
      "24/03/07 01:14:26 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/07 01:14:26 INFO DAGScheduler: running: Set()\n",
      "24/03/07 01:14:26 INFO DAGScheduler: waiting: Set()\n",
      "24/03/07 01:14:26 INFO DAGScheduler: failed: Set()\n",
      "24/03/07 01:14:26 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/03/07 01:14:26 INFO CodeGenerator: Code generated in 11.503143 ms\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Registering RDD 24 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Got map stage job 5 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (javaToPython at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:14:26 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 45.3 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:43613 (size: 19.4 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:14:26 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:14:26 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 5) (172.31.108.238, executor 1, partition 0, NODE_LOCAL, 7773 bytes) \n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.31.108.238:38329 in memory (size: 15.3 KiB, free: 1663.1 MiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:43613 in memory (size: 15.3 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.31.108.238:38329 (size: 19.4 KiB, free: 1663.1 MiB)\n",
      "24/03/07 01:14:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 127.0.0.1:57526\n",
      "24/03/07 01:14:26 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 5) in 228 ms on 172.31.108.238 (executor 1) (1/1)\n",
      "24/03/07 01:14:26 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:14:26 INFO DAGScheduler: ShuffleMapStage 9 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.244 s\n",
      "24/03/07 01:14:26 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/07 01:14:26 INFO DAGScheduler: running: Set()\n",
      "24/03/07 01:14:26 INFO DAGScheduler: waiting: Set()\n",
      "24/03/07 01:14:26 INFO DAGScheduler: failed: Set()\n",
      "24/03/07 01:14:26 INFO CodeGenerator: Code generated in 8.051884 ms\n",
      "24/03/07 01:14:26 INFO CodeGenerator: Code generated in 19.470303 ms\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Registering RDD 35 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 4\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Got map stage job 6 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[35] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:43613 in memory (size: 19.4 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.31.108.238:38329 in memory (size: 19.4 KiB, free: 1663.1 MiB)\n",
      "24/03/07 01:14:26 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 47.9 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:43613 (size: 18.8 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:14:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[35] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:14:26 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:14:26 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 6) (172.31.108.238, executor 1, partition 0, NODE_LOCAL, 7773 bytes) \n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:43613 in memory (size: 35.3 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.31.108.238:38329 in memory (size: 35.3 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:26 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.31.108.238:38329 (size: 18.8 KiB, free: 1663.1 MiB)\n",
      "24/03/07 01:14:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 127.0.0.1:57526\n",
      "24/03/07 01:14:30 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 6) in 3408 ms on 172.31.108.238 (executor 1) (1/1)\n",
      "24/03/07 01:14:30 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:14:30 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37817\n",
      "24/03/07 01:14:30 INFO DAGScheduler: ShuffleMapStage 12 (parquet at NativeMethodAccessorImpl.java:0) finished in 3.434 s\n",
      "24/03/07 01:14:30 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/07 01:14:30 INFO DAGScheduler: running: Set()\n",
      "24/03/07 01:14:30 INFO DAGScheduler: waiting: Set()\n",
      "24/03/07 01:14:30 INFO DAGScheduler: failed: Set()\n",
      "24/03/07 01:14:30 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/03/07 01:14:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/03/07 01:14:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/03/07 01:14:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/03/07 01:14:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/03/07 01:14:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/03/07 01:14:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/03/07 01:14:30 INFO CodeGenerator: Code generated in 13.380691 ms\n",
      "24/03/07 01:14:30 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "24/03/07 01:14:30 INFO DAGScheduler: Got job 7 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:14:30 INFO DAGScheduler: Final stage: ResultStage 16 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:14:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "24/03/07 01:14:30 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:14:30 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[38] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:14:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 223.9 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 78.8 KiB, free 3.4 GiB)\n",
      "24/03/07 01:14:30 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:43613 in memory (size: 18.8 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:43613 (size: 78.8 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:14:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:14:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[38] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:14:30 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:14:30 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.31.108.238:38329 in memory (size: 18.8 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:14:30 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 7) (172.31.108.238, executor 0, partition 0, NODE_LOCAL, 7784 bytes) \n",
      "24/03/07 01:14:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.31.108.238:37061 (size: 78.8 KiB, free: 1663.1 MiB)\n",
      "24/03/07 01:14:30 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 127.0.0.1:57520\n",
      "24/03/07 01:14:30 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 7) in 397 ms on 172.31.108.238 (executor 0) (1/1)\n",
      "24/03/07 01:14:30 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:14:30 INFO DAGScheduler: ResultStage 16 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.425 s\n",
      "24/03/07 01:14:30 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/07 01:14:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "24/03/07 01:14:30 INFO DAGScheduler: Job 7 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.429841 s\n",
      "24/03/07 01:14:30 INFO FileFormatWriter: Start to commit write Job 7fd8d323-88c7-4890-af39-d082a4282b06.\n",
      "24/03/07 01:14:30 INFO FileFormatWriter: Write Job 7fd8d323-88c7-4890-af39-d082a4282b06 committed. Elapsed time: 18 ms.\n",
      "24/03/07 01:14:30 INFO FileFormatWriter: Finished processing stats for write job 7fd8d323-88c7-4890-af39-d082a4282b06.\n",
      "24/03/07 01:14:31 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "24/03/07 01:14:31 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/03/07 01:14:31 INFO SparkUI: Stopped Spark web UI at http://localhost:4040\n",
      "24/03/07 01:14:31 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/03/07 01:14:31 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/03/07 01:14:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/03/07 01:14:31 INFO MemoryStore: MemoryStore cleared\n",
      "24/03/07 01:14:31 INFO BlockManager: BlockManager stopped\n",
      "24/03/07 01:14:31 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/03/07 01:14:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/03/07 01:14:31 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/03/07 01:14:31 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/03/07 01:14:31 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-b0dd0425-042d-48ab-98c0-881a27616ff2\n",
      "24/03/07 01:14:31 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-4cf6689d-3706-417f-8b0f-f93628a8c81d\n",
      "24/03/07 01:14:31 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-2e0811aa-9eff-4ebc-ae63-5f0ef175b47b\n",
      "24/03/07 01:14:31 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-2e0811aa-9eff-4ebc-ae63-5f0ef175b47b/pyspark-d4a60ba2-c86f-49fc-9998-9357c9de4a46\n"
     ]
    }
   ],
   "source": [
    "!SETU_DIR=/home/$USER/setu SETU_TMP_DIR=/home/$USER/tmp/ FILTER_DATA_ROOT=/home/$USER/setu/setu/data \\\n",
    "    spark-submit \\\n",
    "    --master spark://YDEARYZEN.:7077 \\\n",
    "    --deploy-mode client \\\n",
    "    --driver-java-options -Djava.io.tmpdir=/home/$USER/tmp/ \\\n",
    "    --conf \"spark.driver.extraJavaOptions=-Djava.io.tmpdir=/home/$USER/tmp/\" \\\n",
    "    --conf \"spark.executor.extraJavaOptions=-Djava.io.tmpdir=/home/$USER/tmp/\" \\\n",
    "    --conf spark.worker.dir=\"/home/$USER/tmp/\" \\\n",
    "    --conf spark.local.dir=\"/home/$USER/tmp/\" \\\n",
    "    --num-executors 4 \\\n",
    "    --executor-cores 2 \\\n",
    "    --executor-memory 3G \\\n",
    "    --driver-memory 6G \\\n",
    "    --archives \"/home/$USER/setu/dataproc/envs/setu.zip\" \\\n",
    "    --conf 'spark.executorEnv.PYTHONPATH=setu.zip' \\\n",
    "    --conf 'spark.executorEnv.FILTER_DATA_ROOT=setu.zip/data' \\\n",
    "    /home/$USER/setu/setu/run.py \\\n",
    "    --config /home/$USER/setu/configs/crawls/spark_english_config.json \\\n",
    "    --mode crawl \\\n",
    "    --run_local True \\\n",
    "    ExtractTextStage \\\n",
    "    --te_parquets_path \"/home/$USER/setu/examples/output/j2p_output/*.parquet\" \\\n",
    "    --te_samples_per_partition 1500 \\\n",
    "    --te_run_mode data \\\n",
    "    --te_output_path \"/home/$USER/setu/examples/output/te_output\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the TextExtract Stage Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_output_path = f\"/home/{os.environ.get('USER')}/setu/examples/output/te_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquets = glob(f\"{te_output_path}/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(parquets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>successful_extraction</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "      <th>comments</th>\n",
       "      <th>author</th>\n",
       "      <th>...</th>\n",
       "      <th>categories</th>\n",
       "      <th>tags</th>\n",
       "      <th>fingerprint</th>\n",
       "      <th>id</th>\n",
       "      <th>license</th>\n",
       "      <th>body</th>\n",
       "      <th>commentsbody</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>image</th>\n",
       "      <th>pagetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8bc919b9-c288-42fd-b503-d0f366e06a14</td>\n",
       "      <td>https://www.91mobiles.com/nokia-t10-lte-price-...</td>\n",
       "      <td>91mobiles</td>\n",
       "      <td>10/07/23 10:54</td>\n",
       "      <td>True</td>\n",
       "      <td>Nokia T10 LTE</td>\n",
       "      <td>None</td>\n",
       "      <td>1. Trusted Brand\\n2. Originally from Finland, ...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 doc_id  \\\n",
       "0  8bc919b9-c288-42fd-b503-d0f366e06a14   \n",
       "\n",
       "                                                 url     source  \\\n",
       "0  https://www.91mobiles.com/nokia-t10-lte-price-...  91mobiles   \n",
       "\n",
       "        timestamp  successful_extraction          title description  \\\n",
       "0  10/07/23 10:54                   True  Nokia T10 LTE        None   \n",
       "\n",
       "                                                text comments author  ...  \\\n",
       "0  1. Trusted Brand\\n2. Originally from Finland, ...            None  ...   \n",
       "\n",
       "  categories tags fingerprint    id license  body commentsbody raw_text image  \\\n",
       "0         []   []        None  None    None  None         None     None  None   \n",
       "\n",
       "  pagetype  \n",
       "0     None  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run DocCleanStage on the Text Extraction Stage output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/07 01:55:17 WARN Utils: Your hostname, YDEARYZEN resolves to a loopback address: 127.0.1.1; using 172.31.108.238 instead (on interface eth0)\n",
      "24/03/07 01:55:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/07 01:55:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Command used to run this script:  /home/shanks/setu/setu/run.py --config /home/shanks/setu/configs/crawls/spark_english_config.json --mode crawl --run_local True DocCleanStage --doc_df_parquets_path /home/shanks/setu/examples/output/te_output/*.parquet --is_doc_df_path_batched False --doc_clean_additional_cols_to_use url,source,language --use_symbol_filter True --doc_clean_samples_per_partition 1500 --doc_clean_verbose False --doc_clean_run_mode data --save_symbol_heavy_docs True --symbol_filter_output_path /home/shanks/setu/examples/output/symbol_filter/ --cleaned_doc_output_path /home/shanks/setu/examples/output/cleaned_docs/\n",
      "shanks\n",
      "------------------------------------------------ Setting Environment Variables --------------------------------------------------\n",
      "------------------------------------------------ END --------------------------------------------------\n",
      "------------------------------------------------ Environment Variables --------------------------------------------------\n",
      "PATH: /home/shanks/miniconda3/envs/setu_env/bin:/home/shanks/.vscode-server/bin/019f4d1419fbc8219a181fab7892ebccf7ee29a2/bin/remote-cli:/home/shanks/hadoop/spark-3.5.1/bin:/home/shanks/miniconda3/envs/setu_env/bin:/home/shanks/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files (x86)/VMware/VMware Player/bin:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/dotnet:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files/Go/bin:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/Program Files/RabbitMQ Server/rabbitmq_server-3.12.13/sbin:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311/Scripts:/mnt/c/Users/Shanks/AppData/Local/Programs/Python/Python311:/mnt/c/Users/Shanks/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Shanks/AppData/Roaming/npm:/mnt/c/Users/Shanks/AppData/Local/Google/Cloud SDK/google-cloud-sdk/bin:/mnt/c/Users/Shanks/miniconda3/Scripts:/mnt/c/Users/Shanks/miniconda3:/mnt/c/Users/Shanks/miniconda3/Library/bin/conda.bat:/snap/bin\n",
      "XDG_DATA_DIRS: /usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "CONDA_DEFAULT_ENV: setu_env\n",
      "GIT_PAGER: cat\n",
      "CLICOLOR_FORCE: 1\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "SPARK_ENV_LOADED: 1\n",
      "CONDA_PYTHON_EXE: /home/shanks/miniconda3/bin/python\n",
      "MOTD_SHOWN: update-motd\n",
      "WSL2_GUI_APPS_ENABLED: 1\n",
      "DBUS_SESSION_BUS_ADDRESS: unix:path=/run/user/1000/bus\n",
      "FILTER_DATA_ROOT: /home/shanks/setu/setu/data\n",
      "CONDA_PREFIX: /home/shanks/miniconda3/envs/setu_env\n",
      "CONDA_PREFIX_1: /home/shanks/miniconda3\n",
      "LOGNAME: shanks\n",
      "SETU_TMP_DIR: /home/shanks/tmp/\n",
      "PWD: /home/shanks/setu/examples\n",
      "VSCODE_NLS_CONFIG: {\"locale\":\"en\",\"osLocale\":\"en\",\"availableLanguages\":{}}\n",
      "PYTHONPATH: /home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip:/home/shanks/hadoop/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip:/home/shanks/hadoop/spark-3.5.1/jars/spark-core_2.12-3.5.1.jar\n",
      "LESSOPEN: | /usr/bin/lesspipe %s\n",
      "SHELL: /bin/bash\n",
      "WSL_INTEROP: /run/WSL/379_interop\n",
      "CONDA_ROOT: /home/shanks/miniconda3\n",
      "PAGER: cat\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "HOSTTYPE: x86_64\n",
      "VSCODE_WSL_EXT_LOCATION: /mnt/c/Users/Shanks/.vscode/extensions/ms-vscode-remote.remote-wsl-0.86.0\n",
      "SPARK_CONF_DIR: /home/shanks/hadoop/spark-3.5.1/conf\n",
      "NAME: YDEARYZEN\n",
      "CONDA_PROMPT_MODIFIER: (setu_env) \n",
      "LS_COLORS: rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\n",
      "SHLVL: 1\n",
      "GSETTINGS_SCHEMA_DIR_CONDA_BACKUP: \n",
      "SETU_DIR: /home/shanks/setu\n",
      "VSCODE_HANDLES_SIGPIPE: true\n",
      "LESSCLOSE: /usr/bin/lesspipe %s %s\n",
      "FORCE_COLOR: 1\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "CONDA_EXE: /home/shanks/miniconda3/bin/conda\n",
      "TERM: xterm-color\n",
      "LANG: C.UTF-8\n",
      "SPARK_SCALA_VERSION: 2.12\n",
      "DISPLAY: :0\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "SPARK_HOME: /home/shanks/hadoop/spark-3.5.1\n",
      "_CE_M: \n",
      "WAYLAND_DISPLAY: wayland-0\n",
      "VSCODE_IPC_HOOK_CLI: /run/user/1000/vscode-ipc-27ddf6b5-ef93-444d-97b6-5692a863b9a1.sock\n",
      "VSCODE_AMD_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "CONDA_SHLVL: 2\n",
      "PYTHONHASHSEED: 0\n",
      "VSCODE_CWD: /mnt/c/Users/Shanks/AppData/Local/Programs/Microsoft VS Code\n",
      "WSL_DISTRO_NAME: Ubuntu\n",
      "PYTHONIOENCODING: utf-8\n",
      "PYSPARK_GATEWAY_SECRET: f3fff9c1a64708b3c9c8ba2bc114d4f767719b75d321f224a59f491c494be755\n",
      "USER: shanks\n",
      "CLICOLOR: 1\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "PULSE_SERVER: unix:/mnt/wslg/PulseServer\n",
      "PYSPARK_GATEWAY_PORT: 35295\n",
      "_CE_CONDA: \n",
      "PYTHONUNBUFFERED: YES\n",
      "WSLENV: VSCODE_WSL_EXT_LOCATION/up\n",
      "GSETTINGS_SCHEMA_DIR: /home/shanks/miniconda3/envs/setu_env/share/glib-2.0/schemas\n",
      "XDG_RUNTIME_DIR: /run/user/1000/\n",
      "HOME: /home/shanks\n",
      "------------------------------------------------ END --------------------------------------------------\n",
      "------------------------------------------------ Directory Structure --------------------------------------------------\n",
      "commands.md\n",
      "demo.ipynb\n",
      "my_commands.md\n",
      "output\n",
      "sample_data\n",
      "ls: cannot access 'setu.zip': No such file or directory\n",
      "------------------------------------------------ End --------------------------------------------------\n",
      "Command used to run this script:  /home/shanks/setu/setu/run.py --config /home/shanks/setu/configs/crawls/spark_english_config.json --mode crawl --run_local True DocCleanStage --doc_df_parquets_path /home/shanks/setu/examples/output/te_output/*.parquet --is_doc_df_path_batched False --doc_clean_additional_cols_to_use url,source,language --use_symbol_filter True --doc_clean_samples_per_partition 1500 --doc_clean_verbose False --doc_clean_run_mode data --save_symbol_heavy_docs True --symbol_filter_output_path /home/shanks/setu/examples/output/symbol_filter/ --cleaned_doc_output_path /home/shanks/setu/examples/output/cleaned_docs/\n",
      "Entered Commandline Arguments:  Namespace(config='/home/shanks/setu/configs/crawls/spark_english_config.json', mode='crawl', run_local=True, stage='DocCleanStage', doc_df_parquets_path='/home/shanks/setu/examples/output/te_output/*.parquet', is_doc_df_path_batched=False, doc_clean_additional_cols_to_use=['url', 'source', 'language'], use_symbol_filter=True, doc_clean_samples_per_partition=1500, doc_clean_verbose=False, doc_clean_run_mode='data', save_symbol_heavy_docs=True, symbol_filter_output_path='/home/shanks/setu/examples/output/symbol_filter/', cleaned_doc_output_path='/home/shanks/setu/examples/output/cleaned_docs/')\n",
      "PySpark Installed. Going forward with spark execution for JSON2ParquetStage\n",
      "PySpark Installed. Going forward with spark execution for ExtractTextStage\n",
      "PySpark Installed. Going forward with spark execution for FormatConversionStage\n",
      "PySpark Installed. Going forward with spark execution for PageAnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for PageFilteringStage\n",
      "PySpark Installed. Going forward with spark execution for PageTextExtractionStage\n",
      "PySpark Installed. Going forward with spark execution for PageMergeStage\n",
      "PySpark Installed. Going forward with spark execution for DocCleanStage\n",
      "PySpark Installed. Going forward with spark execution for LIDStage\n",
      "PySpark Installed. Going forward with spark execution for AnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for FlaggingAndFilteringStage\n",
      "PySpark Installed. Going forward with spark execution for DocumentRemovalStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeJson2ParquetStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeDocCleanStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeLIDStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeAnalysisStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeFilterStage\n",
      "PySpark Installed. Going forward with spark execution for VisualizeMinhashStage\n",
      "PySpark Installed. Going forward with spark execution for CustomJoinStage\n",
      "PySpark Installed. Going forward with spark execution for Page2PDFStage\n",
      "PySpark Installed. Going forward with spark execution for NormalizeStage\n",
      "24/03/07 01:55:22 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/03/07 01:55:22 INFO SparkContext: OS info Linux, 5.15.133.1-microsoft-standard-WSL2, amd64\n",
      "24/03/07 01:55:22 INFO SparkContext: Java version 11.0.22\n",
      "24/03/07 01:55:22 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/03/07 01:55:22 INFO ResourceUtils: ==============================================================\n",
      "24/03/07 01:55:22 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/03/07 01:55:22 INFO ResourceUtils: ==============================================================\n",
      "24/03/07 01:55:22 INFO SparkContext: Submitted application: Sangraha Anaylsis\n",
      "24/03/07 01:55:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 3072, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/03/07 01:55:22 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "24/03/07 01:55:22 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/03/07 01:55:22 INFO SecurityManager: Changing view acls to: shanks\n",
      "24/03/07 01:55:22 INFO SecurityManager: Changing modify acls to: shanks\n",
      "24/03/07 01:55:22 INFO SecurityManager: Changing view acls groups to: \n",
      "24/03/07 01:55:22 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/03/07 01:55:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: shanks; groups with view permissions: EMPTY; users with modify permissions: shanks; groups with modify permissions: EMPTY\n",
      "24/03/07 01:55:22 INFO Utils: Successfully started service 'sparkDriver' on port 43167.\n",
      "24/03/07 01:55:22 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/03/07 01:55:22 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/03/07 01:55:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/03/07 01:55:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/03/07 01:55:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/03/07 01:55:22 INFO DiskBlockManager: Created local directory at /home/shanks/tmp/blockmgr-19965664-18c2-4087-b138-a8261c602ecc\n",
      "24/03/07 01:55:22 INFO MemoryStore: MemoryStore started with capacity 3.4 GiB\n",
      "24/03/07 01:55:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/03/07 01:55:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/03/07 01:55:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/03/07 01:55:22 INFO SparkContext: Added archive file:///home/shanks/setu/dataproc/envs/setu.zip at spark://localhost:43167/files/setu.zip with timestamp 1709756722331\n",
      "24/03/07 01:55:22 INFO Utils: Copying /home/shanks/setu/dataproc/envs/setu.zip to /home/shanks/tmp/spark-120eafd1-2de5-496a-b4b2-beb05f37c29b/setu.zip\n",
      "24/03/07 01:55:26 INFO SparkContext: Unpacking an archive file:///home/shanks/setu/dataproc/envs/setu.zip from /home/shanks/tmp/spark-120eafd1-2de5-496a-b4b2-beb05f37c29b/setu.zip to /home/shanks/tmp/spark-c2bb3150-8307-40cd-b8b5-d98ebae29780/userFiles-1dd994db-bd30-4bfe-801f-5384f5a6fc8c/setu.zip\n",
      "24/03/07 01:55:37 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://YDEARYZEN.:7077...\n",
      "24/03/07 01:55:37 INFO TransportClientFactory: Successfully created connection to YDEARYZEN./127.0.1.1:7077 after 20 ms (0 ms spent in bootstraps)\n",
      "24/03/07 01:55:38 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240307015538-0004\n",
      "24/03/07 01:55:38 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240307015538-0004/0 on worker-20240307011034-172.31.108.238-44751 (172.31.108.238:44751) with 2 core(s)\n",
      "24/03/07 01:55:38 INFO StandaloneSchedulerBackend: Granted executor ID app-20240307015538-0004/0 on hostPort 172.31.108.238:44751 with 2 core(s), 3.0 GiB RAM\n",
      "24/03/07 01:55:38 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240307015538-0004/1 on worker-20240307011034-172.31.108.238-44751 (172.31.108.238:44751) with 2 core(s)\n",
      "24/03/07 01:55:38 INFO StandaloneSchedulerBackend: Granted executor ID app-20240307015538-0004/1 on hostPort 172.31.108.238:44751 with 2 core(s), 3.0 GiB RAM\n",
      "24/03/07 01:55:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41785.\n",
      "24/03/07 01:55:38 INFO NettyBlockTransferService: Server created on localhost:41785\n",
      "24/03/07 01:55:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/03/07 01:55:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 41785, None)\n",
      "24/03/07 01:55:38 INFO BlockManagerMasterEndpoint: Registering block manager localhost:41785 with 3.4 GiB RAM, BlockManagerId(driver, localhost, 41785, None)\n",
      "24/03/07 01:55:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 41785, None)\n",
      "24/03/07 01:55:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 41785, None)\n",
      "24/03/07 01:55:38 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240307015538-0004/1 is now RUNNING\n",
      "24/03/07 01:55:38 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240307015538-0004/0 is now RUNNING\n",
      "24/03/07 01:55:38 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "Setu: running `DocCleanStage` stage/component\n",
      "24/03/07 01:55:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/03/07 01:55:38 INFO SharedState: Warehouse path is 'file:/home/shanks/setu/examples/spark-warehouse'.\n",
      "24/03/07 01:55:39 INFO InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.\n",
      "24/03/07 01:55:39 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "24/03/07 01:55:39 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/07 01:55:39 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "24/03/07 01:55:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/07 01:55:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/07 01:55:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/07 01:55:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 104.1 KiB, free 3.4 GiB)\n",
      "24/03/07 01:55:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 37.7 KiB, free 3.4 GiB)\n",
      "24/03/07 01:55:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:41785 (size: 37.7 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:55:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/07 01:55:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/07 01:55:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/03/07 01:55:40 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:36888) with ID 0,  ResourceProfileId 0\n",
      "24/03/07 01:55:41 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.108.238:38233 with 1663.2 MiB RAM, BlockManagerId(0, 172.31.108.238, 38233, None)\n",
      "24/03/07 01:55:41 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:36904) with ID 1,  ResourceProfileId 0\n",
      "24/03/07 01:55:41 INFO BlockManagerMasterEndpoint: Registering block manager 172.31.108.238:33933 with 1663.2 MiB RAM, BlockManagerId(1, 172.31.108.238, 33933, None)\n",
      "24/03/07 01:56:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.31.108.238, executor 0, partition 0, PROCESS_LOCAL, 7958 bytes) \n",
      "24/03/07 01:56:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.31.108.238:38233 (size: 37.7 KiB, free: 1663.2 MiB)\n",
      "24/03/07 01:56:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 847 ms on 172.31.108.238 (executor 0) (1/1)\n",
      "24/03/07 01:56:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/07 01:56:52 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 73.075 s\n",
      "24/03/07 01:56:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/07 01:56:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/03/07 01:56:52 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 73.103533 s\n",
      "24/03/07 01:56:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:41785 in memory (size: 37.7 KiB, free: 3.4 GiB)\n",
      "24/03/07 01:56:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.31.108.238:38233 in memory (size: 37.7 KiB, free: 1663.2 MiB)\n",
      "Encountered an Error:  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `language` cannot be resolved. Did you mean one of the following? [`image`, `author`, `date`, `source`, `tags`].;\n",
      "'Project [doc_id#0, text#7, url#1, source#2, 'language]\n",
      "+- Deduplicate [doc_id#0]\n",
      "   +- Relation [doc_id#0,url#1,source#2,timestamp#3,successful_extraction#4,title#5,description#6,text#7,comments#8,author#9,hostname#10,sitename#11,date#12,categories#13,tags#14,fingerprint#15,id#16,license#17,body#18,commentsbody#19,raw_text#20,image#21,pagetype#22] parquet\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shanks/setu/setu/run.py\", line 57, in <module>\n",
      "    setu.run(spark, stage, **args)\n",
      "  File \"/home/shanks/setu/setu/setu.py\", line 138, in run\n",
      "    return self.run_component(spark=spark, component=component, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/setu.py\", line 132, in run_component\n",
      "    return self.components[self._stage_component_mapping[component]].run(spark=spark, stage=component, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/crawl_analysis.py\", line 63, in run\n",
      "    return self.run_stage(spark=spark, stage=stage, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/crawl_analysis.py\", line 57, in run_stage\n",
      "    return self.stages[stage].run(spark, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/doc_clean.py\", line 579, in run\n",
      "    return self.run_spark(spark, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/doc_clean.py\", line 560, in run_spark\n",
      "    return self.run_data_parallelized(\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/doc_clean.py\", line 455, in run_data_parallelized\n",
      "    df = self.run_preprocessing(\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/doc_clean.py\", line 276, in run_preprocessing\n",
      "    df = df.dropDuplicates([doc_id_col]) \\\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 3227, in select\n",
      "    jdf = self._jdf.select(self._jcols(*cols))\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `language` cannot be resolved. Did you mean one of the following? [`image`, `author`, `date`, `source`, `tags`].;\n",
      "'Project [doc_id#0, text#7, url#1, source#2, 'language]\n",
      "+- Deduplicate [doc_id#0]\n",
      "   +- Relation [doc_id#0,url#1,source#2,timestamp#3,successful_extraction#4,title#5,description#6,text#7,comments#8,author#9,hostname#10,sitename#11,date#12,categories#13,tags#14,fingerprint#15,id#16,license#17,body#18,commentsbody#19,raw_text#20,image#21,pagetype#22] parquet\n",
      "\n",
      "24/03/07 01:56:53 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/03/07 01:56:53 INFO SparkUI: Stopped Spark web UI at http://localhost:4040\n",
      "24/03/07 01:56:53 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/03/07 01:56:53 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/03/07 01:56:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/03/07 01:56:53 INFO MemoryStore: MemoryStore cleared\n",
      "24/03/07 01:56:53 INFO BlockManager: BlockManager stopped\n",
      "24/03/07 01:56:53 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/03/07 01:56:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/03/07 01:56:53 INFO SparkContext: Successfully stopped SparkContext\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shanks/setu/setu/run.py\", line 57, in <module>\n",
      "    setu.run(spark, stage, **args)\n",
      "  File \"/home/shanks/setu/setu/setu.py\", line 138, in run\n",
      "    return self.run_component(spark=spark, component=component, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/setu.py\", line 132, in run_component\n",
      "    return self.components[self._stage_component_mapping[component]].run(spark=spark, stage=component, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/crawl_analysis.py\", line 63, in run\n",
      "    return self.run_stage(spark=spark, stage=stage, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/crawl_analysis.py\", line 57, in run_stage\n",
      "    return self.stages[stage].run(spark, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/doc_clean.py\", line 579, in run\n",
      "    return self.run_spark(spark, **kwargs)\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/doc_clean.py\", line 560, in run_spark\n",
      "    return self.run_data_parallelized(\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/doc_clean.py\", line 455, in run_data_parallelized\n",
      "    df = self.run_preprocessing(\n",
      "  File \"/home/shanks/setu/setu/components/crawl_analysis/doc_clean.py\", line 276, in run_preprocessing\n",
      "    df = df.dropDuplicates([doc_id_col]) \\\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 3227, in select\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "  File \"/home/shanks/hadoop/spark-3.5.1/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "pyspark.errors.exceptions.captured.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `language` cannot be resolved. Did you mean one of the following? [`image`, `author`, `date`, `source`, `tags`].;\n",
      "'Project [doc_id#0, text#7, url#1, source#2, 'language]\n",
      "+- Deduplicate [doc_id#0]\n",
      "   +- Relation [doc_id#0,url#1,source#2,timestamp#3,successful_extraction#4,title#5,description#6,text#7,comments#8,author#9,hostname#10,sitename#11,date#12,categories#13,tags#14,fingerprint#15,id#16,license#17,body#18,commentsbody#19,raw_text#20,image#21,pagetype#22] parquet\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shanks/setu/setu/run.py\", line 65, in <module>\n",
      "    raise Exception(\"Job Failed with above mentioned exception\")\n",
      "Exception: Job Failed with above mentioned exception\n",
      "24/03/07 01:56:54 INFO ShutdownHookManager: Shutdown hook called\n",
      "24/03/07 01:56:54 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-c2bb3150-8307-40cd-b8b5-d98ebae29780/pyspark-6954f5bc-7e8f-47cc-868a-99dd1d05ff40\n",
      "24/03/07 01:56:54 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-5116187a-8b9f-4357-a7b2-97830f2d81d0\n",
      "24/03/07 01:56:54 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-c2bb3150-8307-40cd-b8b5-d98ebae29780\n",
      "24/03/07 01:56:54 INFO ShutdownHookManager: Deleting directory /home/shanks/tmp/spark-120eafd1-2de5-496a-b4b2-beb05f37c29b\n"
     ]
    }
   ],
   "source": [
    "!SETU_DIR=/home/$USER/setu SETU_TMP_DIR=/home/$USER/tmp/ FILTER_DATA_ROOT=/home/$USER/setu/setu/data \\\n",
    "    spark-submit \\\n",
    "    --master spark://YDEARYZEN.:7077 \\\n",
    "    --deploy-mode client \\\n",
    "    --driver-java-options -Djava.io.tmpdir=/home/$USER/tmp/ \\\n",
    "    --conf \"spark.driver.extraJavaOptions=-Djava.io.tmpdir=/home/$USER/tmp/\" \\\n",
    "    --conf \"spark.executor.extraJavaOptions=-Djava.io.tmpdir=/home/$USER/tmp/\" \\\n",
    "    --conf spark.worker.dir=\"/home/$USER/tmp/\" \\\n",
    "    --conf spark.local.dir=\"/home/$USER/tmp/\" \\\n",
    "    --num-executors 4 \\\n",
    "    --executor-cores 2 \\\n",
    "    --executor-memory 3G \\\n",
    "    --driver-memory 6G \\\n",
    "    --archives \"/home/$USER/setu/dataproc/envs/setu.zip\" \\\n",
    "    --conf 'spark.executorEnv.PYTHONPATH=setu.zip' \\\n",
    "    --conf 'spark.executorEnv.FILTER_DATA_ROOT=setu.zip/data' \\\n",
    "    /home/$USER/setu/setu/run.py \\\n",
    "    --config /home/$USER/setu/configs/crawls/spark_english_config.json \\\n",
    "    --mode crawl \\\n",
    "    --run_local True \\\n",
    "    DocCleanStage \\\n",
    "    --doc_df_parquets_path \"/home/$USER/setu/examples/output/te_output/*.parquet\" \\\n",
    "    --is_doc_df_path_batched False \\\n",
    "    --doc_clean_additional_cols_to_use \"url,source,language\" \\\n",
    "    --use_symbol_filter True \\\n",
    "    --doc_clean_samples_per_partition 1500 \\\n",
    "    --doc_clean_verbose False \\\n",
    "    --doc_clean_run_mode data \\\n",
    "    --save_symbol_heavy_docs True \\\n",
    "    --symbol_filter_output_path \"/home/$USER/setu/examples/output/symbol_filter/\" \\\n",
    "    --cleaned_doc_output_path \"/home/$USER/setu/examples/output/cleaned_docs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "setu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
