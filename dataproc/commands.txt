gcloud dataproc clusters create test-456 \
    --enable-component-gateway \
    --region asia-south1 \
    --zone asia-south1-a \
    --master-machine-type c2-standard-16 \
    --master-boot-disk-size 500 \
    --num-workers 0 \
    --image 'https://www.googleapis.com/compute/v1/projects/sangraha-396106/global/images/setu-conda-env-dataproc' \
    --metadata conda-default-env=setu_dataproc,startup-script-url=gs://sangraha/setu/dataproc/compute_image/setup_environment.sh

gcloud dataproc clusters create setu-debugging-15 \
    --enable-component-gateway \
    --region asia-south1 \
    --zone asia-south1-a \
    --master-machine-type n2-standard-16 \
    --master-boot-disk-size 500 \
    --num-workers 15 \
    --worker-machine-type n2-standard-16 \
    --worker-boot-disk-size 500 \
    --image-version '2.1-ubuntu20' \
    --properties='dataproc:conda.env.config.uri=gs://sangraha/setu/dataproc/envs/environment.yaml','spark-env:FILTER_DATA_ROOT=/opt/setu/filter_data/filter_data/' \
    --initialization-actions='gs://sangraha/setu/dataproc/compute_image/install_additional_deps.sh'

gcloud dataproc clusters create setu-debugging \
    --enable-component-gateway \
    --region asia-south1 \
    --zone asia-south1-a \
    --master-machine-type n2-standard-16 \
    --master-boot-disk-size 500 \
    --num-workers 0 \
    --image-version '2.1-ubuntu20' \
    --properties='dataproc:conda.env.config.uri=gs://sangraha/setu/dataproc/envs/environment.yaml','spark-env:FILTER_DATA_ROOT=/opt/setu/filter_data/filter_data/' \
    --initialization-actions='gs://sangraha/setu/dataproc/compute_image/install_additional_deps.sh'

gcloud dataproc clusters create ubuntu-567 \
    --enable-component-gateway \
    --region asia-south1 \
    --zone asia-south1-a \
    --master-machine-type c2-standard-16 \
    --master-boot-disk-size 500 \
    --num-workers 0 \
    --image-version '2.1.21-ubuntu20' \
    --properties=^#^dataproc:conda.packages='pytorch==1.7.1,coverage==5.5'#dataproc:pip.packages='tokenizers==0.10.1,datasets==1.5.0'












gcloud dataproc clusters create setu-debugging-0-highmem \
    --enable-component-gateway \
    --region asia-south1 \
    --zone asia-south1-a \
    --master-machine-type n2-highmem-16 \
    --master-boot-disk-size 500 \
    --image-version '2.1-ubuntu20' \
    --properties='dataproc:conda.env.config.uri=gs://sarvam_sangraha/setu/dataproc/envs/environment.yaml','spark-env:FILTER_DATA_ROOT=/opt/setu/filter_data/filter_data/' \
    --initialization-actions='gs://sarvam_sangraha/setu/dataproc/compute_image/install_additional_deps.sh'

gcloud dataproc clusters create setu-debugging-2-highmem \
    --enable-component-gateway \
    --optional-components=Jupyter \
    --region asia-south1 \
    --zone asia-south1-a \
    --master-machine-type n2-highmem-16 \
    --master-boot-disk-size 500 \
    --num-workers 2 \
    --worker-machine-type n2-highmem-16 \
    --worker-boot-disk-size 500 \
    --image-version '2.1-ubuntu20' \
    --properties='dataproc:conda.env.config.uri=gs://sangraha/setu/dataproc/envs/environment.yaml','spark-env:FILTER_DATA_ROOT=/opt/setu/filter_data/filter_data/' \
    --initialization-actions='gs://sangraha/setu/dataproc/compute_image/install_additional_deps.sh'

gcloud dataproc clusters create setu-debugging-5-highmem \
    --enable-component-gateway \
    --region us-central1 \
    --zone us-central1-a \
    --master-machine-type n2-highmem-16 \
    --master-boot-disk-size 500 \
    --num-workers 5 \
    --worker-machine-type n2-highmem-16 \
    --worker-boot-disk-size 500 \
    --image-version '2.1-ubuntu20' \
    --properties='dataproc:conda.env.config.uri=gs://sarvam_sangraha/setu/dataproc/envs/environment.yaml','spark-env:FILTER_DATA_ROOT=/opt/setu/filter_data/filter_data/' \
    --initialization-actions='gs://sarvam_sangraha/setu/dataproc/compute_image/install_additional_deps.sh' \
    --max-idle "10m"

gcloud dataproc clusters create setu-debugging-15-highmem \
    --enable-component-gateway \
    --region us-central1 \
    --zone us-central1-a \
    --master-machine-type n2-highmem-16 \
    --master-boot-disk-size 100 \
    --num-workers 15 \
    --worker-machine-type n2-highmem-16 \
    --worker-boot-disk-size 100 \
    --image-version '2.1-ubuntu20' \
    --properties='dataproc:conda.env.config.uri=gs://sarvam_sangraha/setu/dataproc/envs/environment.yaml','spark-env:FILTER_DATA_ROOT=/opt/setu/filter_data/filter_data/' \
    --initialization-actions='gs://sarvam_sangraha/setu/dataproc/compute_image/install_additional_deps.sh' \
    --max-idle "10m"

gcloud dataproc clusters create setu-debugging-40-highmem \
    --enable-component-gateway \
    --region asia-south1 \
    --zone asia-south1-a \
    --master-machine-type n2-highmem-32 \
    --master-boot-disk-size 100 \
    --num-workers 40 \
    --worker-machine-type n2-highmem-16 \
    --worker-boot-disk-size 100 \
    --image-version '2.1-ubuntu20' \
    --properties='dataproc:conda.env.config.uri=gs://sangraha/setu/dataproc/envs/environment.yaml','spark-env:FILTER_DATA_ROOT=/opt/setu/filter_data/filter_data/' \
    --initialization-actions='gs://sangraha/setu/dataproc/compute_image/install_additional_deps.sh' \
    --max-idle '10m'

gcloud dataproc clusters create setu-debugging-50-highmem \
    --enable-component-gateway \
    --region us-central1 \
    --zone us-central1-a \
    --master-machine-type n2-highmem-32 \
    --master-boot-disk-size 300 \
    --num-workers 50 \
    --worker-machine-type n2-highmem-16 \
    --worker-boot-disk-size 300 \
    --image-version '2.1-ubuntu20' \
    --properties='dataproc:conda.env.config.uri=gs://sarvam_sangraha/setu/dataproc/envs/environment.yaml','spark-env:FILTER_DATA_ROOT=/opt/setu/filter_data/filter_data/' \
    --initialization-actions='gs://sarvam_sangraha/setu/dataproc/compute_image/install_additional_deps.sh' \
    --max-idle '5m'
