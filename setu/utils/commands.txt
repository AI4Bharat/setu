Start Master
~/spark-hadoop3/sbin/start-master.sh

Start a worker
~/spark-hadoop3/sbin/start-worker.sh spark://SPK-DGX-O1:7077  -c 128 -m 512G

Submit a job
spark-submit --master spark://SPK-DGX-O1:7077 \
             --num-executors 12 \
             --executor-cores 4 \
             --executor-memory 15G \
             --driver-memory 10G \
             --py-files setu/constants.py,setu/document_filters.py,setu/line_filters.py,setu/lid.py,setu/utils.py,setu/setu.py \
             setu/run.py \
             --config setu/configs/spark_config.json \
             --parquet_glob_path "../sangraha/parquets/gujarati/*.parquet" \
             --samples_per_partition 20000

spark-submit --master spark://SPK-DGX-O1:7077 \
             --driver-java-options -Djava.io.tmpdir=/data/priyam/tmp/ \
             --conf "spark.driver.extraJavaOptions=-Djava.io.tmpdir=/data/priyam/tmp/" \
             --conf "spark.executor.extraJavaOptions=-Djava.io.tmpdir=/data/priyam/tmp/" \
             --conf spark.worker.dir="/data/priyam/tmp/" \
             --conf spark.local.dir="/data/priyam/tmp/" \
             --num-executors 128 \
             --executor-cores 1 \
             --executor-memory 4G \
             --driver-memory 10G \
             --py-files setu/constants.py,setu/document_filters.py,setu/line_filters.py,setu/lid.py,setu/utils.py,setu/setu.py \
             setu/run.py \
             --config setu/configs/spark_config.json \
             --parquet_glob_path "../sangraha/parquets/gujarati/37.parquet" \
             --samples_per_partition 2000

python -m text_dedup.suffix_array \
    --path "parquet" \
    --name "sangraha-bodo" \
    --split "train" \
    --data_dir "/data/priyam/sangraha/spark_out/bodo/final_docs/*.parquet" \
    --cache_dir "/data/priyam/cache" \
    --output "/data/priyam/sangraha/dedup/bodo" \
    --column "text"             

spark-submit --master spark://e2e-100-5.ssdcloudindia.net:7077 --num-executors 48 --executor-cores 1 --executor-memory 5G --driver-memory 10G ~/setu/utils/convert_to_parquet.py

Stop Master
~/spark-3.4.1-bin-hadoop3/sbin/stop-master.sh

Stop Workers
bash ~/spark-3.4.1-bin-hadoop3/sbin/stop-workers.sh

Keys to keep:
title
author
url
hostname
description
sitename
date
categories
tags
fingerprint
id
license 
body
comments
commentsbody
raw_text
text
language
image
pagetype

240G    ../datasets/sangraha/web_crawls/kannada
274G    ../datasets/sangraha/web_crawls/gujarati
451G    ../datasets/sangraha/web_crawls/english
600G    ../datasets/sangraha/web_crawls/bengali
274G    ../datasets/sangraha/web_crawls/telugu
305G    ../datasets/sangraha/web_crawls/marathi
1.2G    ../datasets/sangraha/web_crawls/bodo
12G     ../datasets/sangraha/web_crawls/sindhi
45G     ../datasets/sangraha/web_crawls/maithili
70G     ../datasets/sangraha/web_crawls/assamese
1.6T    ../datasets/sangraha/web_crawls/hindi
117G    ../datasets/sangraha/web_crawls/odia
423G    ../datasets/sangraha/web_crawls/malayalam
249G    ../datasets/sangraha/web_crawls/nepali
322G    ../datasets/sangraha/web_crawls/tamil
328G    ../datasets/sangraha/web_crawls/urdu
45G     ../datasets/sangraha/web_crawls/sanskrit
126G    ../datasets/sangraha/web_crawls/punjabi
5.4T    ../datasets/sangraha/web_crawls/

70G     ./bengali
86G     ./punjabi
119G    ./english
23G     ./nepali
235G    ./hindi
222G    ./telugu
47G     ./kannada
287G    ./tamil
88G     ./marathi
1.2T    .

6,500,000 docs in malayalam - 423GB - 5hr

1GB = 15366 docs
7TB = 15366 * 7 * 1024

6,500,000 docs = 5hr
~110,143,488 docs = 85hr = ~4 days


