# Filename: crawl_analysis.yaml

jobs:
- pysparkJob:
    archiveUris:
    pythonFileUris:
    mainPythonFileUri: 'path-to-minhash-spark-file-in-gcp'
    args:
    - '--config'
    - 'config json for the language which is going to be processed'
    - '--samples_per_partition'
    - 'no. of samples for each partition to hold so that we don't have OOMs on executors'
    - '--verbose'
    - 'whether to be verbose or not. Only enable this option in local mode'
    - '--checkpoint_dir'
    - 'directory to use for checkpointing and truncating the spark DAG'
    - '--run_data_parallel_mode'
    - 'whether to run data parallel mode for document cleaning stage or not'
    - '--run_doc_clean'
    - 'whether to run document cleaning stage or not'
    - '--doc_df_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for document cleaning stage' 
    - '--is_doc_df_path_batched'
    - 'whether the input path is a batch file or a glob path'
    - '--use_symbol_filter'
    - 'whether to use symbol filter' 
    - '--save_symbol_heavy_docs'
    - 'whether to save symbol heavy documents in a separate folder' 
    - '--symbol_filter_output_path'
    - 'path where symbol heavy documents will be stored' 
    - '--cleaned_doc_output_path'
    - 'path where cleaned documents will be stored' 
    - '--run_lid_segregation'
    - 'whether to run lid segregation stage or not'
    - '--lid_df_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for lid segregation stage'
    - '--is_lid_df_path_batched'
    - 'whether the input path is a batch file or a glob path (for lid segregation stage)'
    - '--doc_lid_output_path'
    - 'path where lid segregated output will be stored'
    - '--run_analysis'
    - 'whether to run analysis stage or not'
    - '--analysis_df_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for analysis stage or document removal stage'
    - '--is_analysis_df_path_batched'
    - 'whether the input path is a batch file or a glob path (for analysis stage)'
    - '--line_stats_output_path'
    - 'path where line stats output will be stored'
    - '--doc_stats_output_path'
    - 'path where doc stats output will be stored'
    - '--analysis_output_path'
    - 'path where analysed documents output will be stored'
    - '--run_flag_and_filter'
    - 'whether to run flagging and filtering stage or not'
    - '--doc_stats_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for flagging and filtering stage'
    - '--is_doc_stats_path_batched'
    - 'whether the input path is a batch file or a glob path (for flagging & filtering stage)'
    - '--save_nsfw_data'
    - 'whether to sotre nsfw data or not'
    - '--nsfw_output_path'
    - 'path where nfsw data will be stored'
    - '--filtered_doc_stats_output_path'
    - 'path where filtered documents stats will stored'
    - '--run_document_removal'
    - 'whether to run document removal stage or not'
    - '--doc_stats_path_for_removal'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for document removal stage'
    - '--filtered_docs_path'
    - 'path where filtered documents stats will stored'
    properties:
      spark.default.parallelism: 'default_value'
      spark.sql.shuffle.partitions: 'default_value'
      spark.sql.execution.arrow.pyspark.enabled: 'default_value'
      spark.sql.adaptive.enabled: 'default_value'
      spark.serializer: 'default_value'
      spark.speculation: 'default_value'
  stepId: DOCUMENT_CLEANING
- pysparkJob:
    archiveUris:
    pythonFileUris:
    mainPythonFileUri: 'path-to-minhash-spark-file-in-gcp'
    args:
    - '--config'
    - 'config json for the language which is going to be processed'
    - '--samples_per_partition'
    - 'no. of samples for each partition to hold so that we don't have OOMs on executors'
    - '--verbose'
    - 'whether to be verbose or not. Only enable this option in local mode'
    - '--checkpoint_dir'
    - 'directory to use for checkpointing and truncating the spark DAG'
    - '--run_data_parallel_mode'
    - 'whether to run data parallel mode for document cleaning stage or not'
    - '--run_doc_clean'
    - 'whether to run document cleaning stage or not'
    - '--doc_df_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for document cleaning stage' 
    - '--is_doc_df_path_batched'
    - 'whether the input path is a batch file or a glob path'
    - '--use_symbol_filter'
    - 'whether to use symbol filter' 
    - '--save_symbol_heavy_docs'
    - 'whether to save symbol heavy documents in a separate folder' 
    - '--symbol_filter_output_path'
    - 'path where symbol heavy documents will be stored' 
    - '--cleaned_doc_output_path'
    - 'path where cleaned documents will be stored' 
    - '--run_lid_segregation'
    - 'whether to run lid segregation stage or not'
    - '--lid_df_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for lid segregation stage'
    - '--is_lid_df_path_batched'
    - 'whether the input path is a batch file or a glob path (for lid segregation stage)'
    - '--doc_lid_output_path'
    - 'path where lid segregated output will be stored'
    - '--run_analysis'
    - 'whether to run analysis stage or not'
    - '--analysis_df_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for analysis stage or document removal stage'
    - '--is_analysis_df_path_batched'
    - 'whether the input path is a batch file or a glob path (for analysis stage)'
    - '--line_stats_output_path'
    - 'path where line stats output will be stored'
    - '--doc_stats_output_path'
    - 'path where doc stats output will be stored'
    - '--analysis_output_path'
    - 'path where analysed documents output will be stored'
    - '--run_flag_and_filter'
    - 'whether to run flagging and filtering stage or not'
    - '--doc_stats_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for flagging and filtering stage'
    - '--is_doc_stats_path_batched'
    - 'whether the input path is a batch file or a glob path (for flagging & filtering stage)'
    - '--save_nsfw_data'
    - 'whether to sotre nsfw data or not'
    - '--nsfw_output_path'
    - 'path where nfsw data will be stored'
    - '--filtered_doc_stats_output_path'
    - 'path where filtered documents stats will stored'
    - '--run_document_removal'
    - 'whether to run document removal stage or not'
    - '--doc_stats_path_for_removal'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for document removal stage'
    - '--filtered_docs_path'
    - 'path where filtered documents stats will stored'
    properties:
      spark.default.parallelism: 'default_value'
      spark.sql.shuffle.partitions: 'default_value'
      spark.sql.execution.arrow.pyspark.enabled: 'default_value'
      spark.sql.adaptive.enabled: 'default_value'
      spark.serializer: 'default_value'
      spark.speculation: 'default_value'
  stepId: LID_SEGREGATION
  prerequisiteStepIds:
  - DOCUMENT_CLEANING
- pysparkJob:
    archiveUris:
    pythonFileUris:
    mainPythonFileUri: 'path-to-minhash-spark-file-in-gcp'
    args:
    - '--config'
    - 'config json for the language which is going to be processed'
    - '--samples_per_partition'
    - 'no. of samples for each partition to hold so that we don't have OOMs on executors'
    - '--verbose'
    - 'whether to be verbose or not. Only enable this option in local mode'
    - '--checkpoint_dir'
    - 'directory to use for checkpointing and truncating the spark DAG'
    - '--run_data_parallel_mode'
    - 'whether to run data parallel mode for document cleaning stage or not'
    - '--run_doc_clean'
    - 'whether to run document cleaning stage or not'
    - '--doc_df_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for document cleaning stage' 
    - '--is_doc_df_path_batched'
    - 'whether the input path is a batch file or a glob path'
    - '--use_symbol_filter'
    - 'whether to use symbol filter' 
    - '--save_symbol_heavy_docs'
    - 'whether to save symbol heavy documents in a separate folder' 
    - '--symbol_filter_output_path'
    - 'path where symbol heavy documents will be stored' 
    - '--cleaned_doc_output_path'
    - 'path where cleaned documents will be stored' 
    - '--run_lid_segregation'
    - 'whether to run lid segregation stage or not'
    - '--lid_df_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for lid segregation stage'
    - '--is_lid_df_path_batched'
    - 'whether the input path is a batch file or a glob path (for lid segregation stage)'
    - '--doc_lid_output_path'
    - 'path where lid segregated output will be stored'
    - '--run_analysis'
    - 'whether to run analysis stage or not'
    - '--analysis_df_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for analysis stage or document removal stage'
    - '--is_analysis_df_path_batched'
    - 'whether the input path is a batch file or a glob path (for analysis stage)'
    - '--line_stats_output_path'
    - 'path where line stats output will be stored'
    - '--doc_stats_output_path'
    - 'path where doc stats output will be stored'
    - '--analysis_output_path'
    - 'path where analysed documents output will be stored'
    - '--run_flag_and_filter'
    - 'whether to run flagging and filtering stage or not'
    - '--doc_stats_parquets_path'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for flagging and filtering stage'
    - '--is_doc_stats_path_batched'
    - 'whether the input path is a batch file or a glob path (for flagging & filtering stage)'
    - '--save_nsfw_data'
    - 'whether to sotre nsfw data or not'
    - '--nsfw_output_path'
    - 'path where nfsw data will be stored'
    - '--filtered_doc_stats_output_path'
    - 'path where filtered documents stats will stored'
    - '--run_document_removal'
    - 'whether to run document removal stage or not'
    - '--doc_stats_path_for_removal'
    - 'parquets glob path or a batch file containing paths to parquets to be processed for document removal stage'
    - '--filtered_docs_path'
    - 'path where filtered documents stats will stored'
    properties:
      spark.default.parallelism: 'default_value'
      spark.sql.shuffle.partitions: 'default_value'
      spark.sql.execution.arrow.pyspark.enabled: 'default_value'
      spark.sql.adaptive.enabled: 'default_value'
      spark.serializer: 'default_value'
      spark.speculation: 'default_value'
  stepId: ANALYSIS
  prerequisiteStepIds:
  - DOCUMENT_CLEANING
  - LID_SEGREGATION

# Cluster configuration
placement:
  managedCluster:
    clusterName: 'to-be-determined'
    config:
      gceClusterConfig:
        zoneUri: asia-south1-a
      masterConfig:
        numInstances: 1
        machineTypeUri: c2d-standard-16
        diskConfig:
          bootDiskSizeGb: 500
      workerConfig:
        numInstances: 10
        machineTypeUri: c2d-standard-16
        diskConfig:
          bootDiskSizeGb: 500
      softwareConfig:
        imageVersion: setu-env

# Define template parameters
parameters:

  - name: MAIN_PYTHON_FILE
    description: The main python file which runs Minhash Spark on GCP
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.mainPythonFileUri
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.mainPythonFileUri

  - name: CLUSTER_NAME
    description: Name of the cluster that is spinned up
    fields:
    - placement.managedCluster.clusterName

  - name: CONFIG
    description: config json for the language which is going to be processed
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[1]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[1]

  - name: SAMPLES_PER_PARTITION
    description: no. of samples for each partition to hold so that we don't have OOMs on executors
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[3]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[3] 

  - name: VERBOSE
    description: whether to be verbose or not. Only enable this option in local mode
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[5]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[5] 

  - name: CHECKPOINT_DIR
    description: directory to use for checkpointing and truncating the spark DAG
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[7]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[7] 

  - name: RUN_DATA_PARALLEL_MODE
    description: whether to run data parallel mode for document cleaning stage or not
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[9]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[9] 

  - name: RUN_DOC_CLEAN
    description: whether to run document cleaning stage or not
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[11]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[11] 

  - name: DOC_DF_PARQUETS_PATH
    description: parquets glob path or a batch file containing paths to parquets to be processed for document cleaning stage
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[13]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[13]  

  - name: IS_DOC_DF_PATH_BATCHED
    description: whether the input path is a batch file or a glob path
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[15]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[15] 

  - name: USE_SYMBOL_FILTER
    description: whether to use symbol filter
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[17]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[17]  

  - name: SAVE_SYMBOL_HEAVY_DOCS
    description: whether to save symbol heavy documents in a separate folder
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[19]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[19]  

  - name: SYMBOL_FILTER_OUTPUT_PATH
    description: path where symbol heavy documents will be stored
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[21]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[21]  

  - name: CLEANED_DOC_OUTPUT_PATH
    description: path where cleaned documents will be stored
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[23]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[23]  

  - name: RUN_LID_SEGREGATION
    description: whether to run lid segregation stage or not
    fields:
    
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[25]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[25]  

  - name: LID_DF_PARQUETS_PATH
    description: parquets glob path or a batch file containing paths to parquets to be processed for lid segregation stage
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[27]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[27]  

  - name: IS_LID_DF_PATH_BATCHED
    description: whether the input path is a batch file or a glob path (for lid segregation stage)
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[29]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[29]  

  - name: DOC_LID_OUTPUT_PATH
    description: path where lid segregated output will be stored
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[31]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[31]  

  - name: RUN_ANALYSIS
    description: whether to run analysis stage or not
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[33]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[33] 

  - name: ANALYSIS_DF_PARQUETS_PATH
    description: parquets glob path or a batch file containing paths to parquets to be processed for analysis stage or document removal stage
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[35]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[35]  

  - name: IS_ANALYSIS_DF_PATH_BATCHED
    description: whether the input path is a batch file or a glob path (for analysis stage)
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[37]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[37]  

  - name: LINE_STATS_OUTPUT_PATH
    description: path where line stats output will be stored
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[39]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[39]  

  - name: DOC_STATS_OUTPUT_PATH
    description: path where doc stats output will be stored
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[41]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[41]  

  - name: ANALYSIS_OUTPUT_PATH
    description: path where analysed documents output will be stored
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[43]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[43]  

  - name: RUN_FLAG_AND_FILTER
    description: whether to run flagging and filtering stage or not
    fields:

    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[45]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[45]

  - name: DOC_STATS_PARQUETS_PATH
    description: parquets glob path or a batch file containing paths to parquets to be processed for flagging and filtering stage
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[47]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[47] 

  - name: IS_DOC_STATS_PATH_BATCHED
    description: whether the input path is a batch file or a glob path (for flagging & filtering stage)
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[49]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[49] 

  - name: SAVE_NSFW_DATA
    description: whether to sotre nsfw data or not
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[51]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[51] 

  - name: NSFW_OUTPUT_PATH
    description: path where nfsw data will be stored
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[53]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[53] 

  - name: FILTERED_DOC_STATS_OUTPUT_PATH
    description: path where filtered documents stats will stored
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[55]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[55] 

  - name: RUN_DOCUMENT_REMOVAL
    description: whether to run document removal stage or not
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[57]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[57] 

  - name: DOC_STATS_PATH_FOR_REMOVAL
    description: parquets glob path or a batch file containing paths to parquets to be processed for document removal stage
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[59]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[59] 

  - name: FILTERED_DOCS_PATH
    description: path where filtered documents stats will stored
    fields:
    - jobs['FLAGGING_AND_FILTERING'].pysparkJob.args[61]
    - jobs['DOCUMENT_REMOVAL'].pysparkJob.args[61] 