Start Master
~/spark-hadoop3/sbin/start-master.sh

Start a worker
~/spark-hadoop3/sbin/start-worker.sh spark://SPK-DGX-O1:7077  -c 128 -m 512G

~/spark-hadoop3/sbin/start-worker.sh spark://SPK-DGX-O2:7077  -c 128 -m 512G

Run SETU bash

FOR Doc Clean

SETU_DIR=/data/priyam/setu SETU_TMP_DIR=/data/priyam/tmp/ bash setu/scripts/run_setu.sh \
    -i english \
    -d "/data/priyam/sangraha/parquets/english/*/*.parquet" \
    -p 6000 \
    -s /data/priyam/sangraha/spark_out/english/dataset/cleaned_docs \
    -b false \
    -c /data/priyam/tmp \
    -u true \
    -l false \
    -a false \
    -f false \
    -t true \
    -r false \
    -v false \
    -n 16 \
    -o 8 \
    -e 32G \
    -k 50G \
    -x 128

FOR Doc Clean

SETU_DIR=/data/priyam/setu SETU_TMP_DIR=/data/priyam/tmp/ bash setu/scripts/run_setu.sh \
    -i malayalam \
    -m "/data/priyam/sangraha/spark_out/malayalam/dataset/batch_infos/doc_clean/batchs.info" \
    -p 1500 \
    -s /data/priyam/sangraha/spark_out/malayalam/dataset/cleaned_docs \
    -b false \
    -c /data/priyam/tmp \
    -u true\
    -l false \
    -a false \
    -f false \
    -t true \
    -r false \
    -v false \
    -n 12 \
    -o 8 \
    -e 32G \
    -k 32G \
    -x 96

FOR LID Segregation

SETU_DIR=/data/priyam/setu SETU_TMP_DIR=/data/priyam/tmp/ bash setu/scripts/run_setu.sh \
    -i punjabi \
    -d "/data/priyam/sangraha/parquets/punjabi/*/*.parquet" \
    -p 1500 \
    -s /data/priyam/sangraha/spark_out/punjabi/dataset/lid_segregation \
    -b false \
    -c /data/priyam/tmp \
    -u false \
    -l true \
    -a false \
    -f false \
    -t true \
    -r false \
    -v false \
    -n 12 \
    -o 8 \
    -e 32G \
    -k 32G \
    -x 96

FOR LID Segregation

SETU_DIR=/data/priyam/setu SETU_TMP_DIR=/data/priyam/tmp/ bash setu/scripts/run_setu.sh \
    -i marathi \
    -m "/data/priyam/sangraha/spark_out/marathi/dataset/batch_infos/lid_segregation/batchs.info" \
    -p 1500 \
    -s /data/priyam/sangraha/spark_out/marathi/dataset/lid_segregation \
    -b false \
    -c /data/priyam/tmp \
    -u false \
    -l true \
    -a false \
    -f false \
    -t true \
    -r false \
    -v false \
    -n 12 \
    -o 8 \
    -e 32G \
    -k 32G \
    -x 96

FOR Analysis

# Desired glob pattern
/data/priyam/sangraha/spark_out/bodo/1/doc_lid/*/*/*.parquet

SETU_DIR=/data/priyam/setu SETU_TMP_DIR=/data/priyam/tmp/ bash setu/scripts/run_setu.sh \
    -i marathi \
    -d "/data/priyam/sangraha/spark_out/marathi/dataset/lid_segregation/doc_lid/*/*/*.parquet" \
    -p 1500 \
    -s /data/priyam/sangraha/spark_out/marathi/dataset/analysis \
    -b false \
    -c /data/priyam/tmp \
    -u false \
    -l false \
    -a true \
    -f false \
    -t true \
    -r false \
    -v false \
    -n 16 \
    -o 8 \
    -e 32G \
    -k 32G \
    -x 128

FOR Analysis

# Desired glob pattern

SETU_DIR=/data/priyam/setu SETU_TMP_DIR=/data/priyam/tmp/ bash setu/scripts/run_setu.sh \
    -i malayalam \
    -m "/data/priyam/sangraha/spark_out/malayalam/dataset/batch_infos/analysis/batchs.info" \
    -p 1500 \
    -s /data/priyam/sangraha/spark_out/malayalam/dataset/analysis \
    -b false \
    -c /data/priyam/tmp \
    -u false \
    -l false \
    -a true \
    -f false \
    -t true \
    -r false \
    -v false \
    -n 12 \
    -o 8 \
    -e 32G \
    -k 32G \
    -x 96

FOR Plotting # todo

FOR Flagging-Filtering

# Desired glob pattern
/data/priyam/sangraha/spark_out/*/*/doc_stats/*/dogri/*.parquet

SETU_DIR=/data/priyam/setu SETU_TMP_DIR=/data/priyam/tmp/ bash setu/scripts/run_setu.sh \
    -i kannada \
    -d "/data/priyam/sangraha/spark_out/kannada/*/analysis/doc_stats/*/kannada/*.parquet" \
    -p 1500 \
    -s /data/priyam/sangraha/spark_out/kannada/dataset/filtering \
    -b false \
    -c /data/priyam/tmp \
    -u false \
    -l false \
    -a false \
    -f true \
    -t true \
    -r false \
    -v false \
    -n 12 \
    -o 8 \
    -e 32G \
    -k 32G \
    -x 96

FOR Document Removal

# Desired glob pattern
analysis = /data/priyam/sangraha/spark_out/gujarati/*/analysis/analysis/*/gujarati/*.parquet
doc_stats = /data/priyam/sangraha/spark_out/gujarati/filtering/filtered_doc_stats/*/*.parquet

SETU_DIR=/data/priyam/setu SETU_TMP_DIR=/data/priyam/tmp/ bash setu/scripts/run_setu.sh \
    -i kannada \
    -d "/data/priyam/sangraha/spark_out/kannada/dataset/analysis/analysis/*/kannada/*.parquet" \
    -p 1500 \
    -s /data/priyam/sangraha/spark_out/kannada/dataset/filtered_docs \
    -b false \
    -c /data/priyam/tmp \
    -u false \
    -l false \
    -a false \
    -f false \
    -t true \
    -r true \
    -q "/data/priyam/sangraha/spark_out/kannada/dataset/filtering/filtered_doc_stats/*/*.parquet" \
    -v false \
    -n 12 \
    -o 8 \
    -e 32G \
    -k 32G \
    -x 96


Submit a job
spark-submit --master spark://SPK-DGX-O1:7077 \
             --num-executors 12 \
             --executor-cores 4 \
             --executor-memory 15G \
             --driver-memory 10G \
             --py-files setu/constants.py,setu/document_filters.py,setu/line_filters.py,setu/lid.py,setu/utils.py,setu/setu.py \
             setu/run.py \
             --config setu/configs/spark_config.json \
             --parquet_glob_path "../sangraha/parquets/gujarati/*.parquet" \
             --samples_per_partition 20000

spark-submit --master spark://SPK-DGX-O1:7077 \
             --driver-java-options -Djava.io.tmpdir=/data/priyam/tmp/ \
             --conf "spark.driver.extraJavaOptions=-Djava.io.tmpdir=/data/priyam/tmp/" \
             --conf "spark.executor.extraJavaOptions=-Djava.io.tmpdir=/data/priyam/tmp/" \
             --conf spark.worker.dir="/data/priyam/tmp/" \
             --conf spark.local.dir="/data/priyam/tmp/" \
             --num-executors 128 \
             --executor-cores 1 \
             --executor-memory 4G \
             --driver-memory 10G \
             --py-files setu/constants.py,setu/document_filters.py,setu/line_filters.py,setu/lid.py,setu/utils.py,setu/setu.py \
             setu/run.py \
             --config setu/configs/spark_config.json \
             --parquet_glob_path "../sangraha/parquets/gujarati/37.parquet" \
             --samples_per_partition 2000

python -m text_dedup.suffix_array \
    --path "parquet" \
    --name "sangraha-assamese" \
    --split "train" \
    --data_files "/data/priyam/sangraha/spark_out/assamese/filtered_docs/dataset/*.parquet" \
    --cache_dir "/data/priyam/cache" \
    --output "/data/priyam/sangraha/dedup/assamese" \
    --column "text" \
    --google_repo_path "/data/priyam/setu/text-dedup/deduplicate-text-datasets"  

python -m text_dedup.minhash \
    --path "parquet" \
    --name "sangraha-bodo" \
    --split "train" \
    --data_files "/data/priyam/sangraha/spark_out/bodo/*/filtered_docs/filtered_docs/*/*.parquet" \
    --cache_dir "/data/priyam/cache" \
    --output "/data/priyam/sangraha/dedup/minhash/bodo" \
    --column "text" \
    --batch_size 10000

--data_dir "/data/priyam/sangraha/spark_out/bodo/filtered_docs/dataset/*.parquet" \ 

spark-submit --master spark://e2e-100-5.ssdcloudindia.net:7077 --num-executors 48 --executor-cores 1 --executor-memory 5G --driver-memory 10G ~/setu/utils/convert_to_parquet.py

Stop Master
~/spark-3.4.1-bin-hadoop3/sbin/stop-master.sh

Stop Workers
bash ~/spark-3.4.1-bin-hadoop3/sbin/stop-workers.sh

Keys to keep:
title
author
url
hostname
description
sitename
date
categories
tags
fingerprint
id
license 
body
comments
commentsbody
raw_text
text
language
image
pagetype

240G    ../datasets/sangraha/web_crawls/kannada
274G    ../datasets/sangraha/web_crawls/gujarati
451G    ../datasets/sangraha/web_crawls/english
600G    ../datasets/sangraha/web_crawls/bengali
274G    ../datasets/sangraha/web_crawls/telugu
305G    ../datasets/sangraha/web_crawls/marathi
1.2G    ../datasets/sangraha/web_crawls/bodo
12G     ../datasets/sangraha/web_crawls/sindhi
45G     ../datasets/sangraha/web_crawls/maithili
70G     ../datasets/sangraha/web_crawls/assamese
1.6T    ../datasets/sangraha/web_crawls/hindi
117G    ../datasets/sangraha/web_crawls/odia
423G    ../datasets/sangraha/web_crawls/malayalam
249G    ../datasets/sangraha/web_crawls/nepali
322G    ../datasets/sangraha/web_crawls/tamil
328G    ../datasets/sangraha/web_crawls/urdu
45G     ../datasets/sangraha/web_crawls/sanskrit
126G    ../datasets/sangraha/web_crawls/punjabi
5.4T    ../datasets/sangraha/web_crawls/

70G     ./bengali
86G     ./punjabi
119G    ./english
23G     ./nepali
235G    ./hindi
222G    ./telugu
47G     ./kannada
287G    ./tamil
88G     ./marathi
1.2T    .

6,500,000 docs in malayalam - 423GB - 5hr

1GB = 15366 docs
7TB = 15366 * 7 * 1024

6,500,000 docs = 5hr
~110,143,488 docs = 85hr = ~4 days


In India, there are 22 official languages and `english`. They are listed below:
- assamese
- bengali
- bodo
- dogri
- english
- gujarati
- hindi
- kannada
- kashmiri
- konkani
- maithili
- malayalam
- marathi
- manipuri
- nepali
- oriya
- punjabi
- sanskrit
- santhali
- sindhi
- tamil
- telugu
- urdu

Give me a python script which takes a string has input and flags any character that is not a alphabet of any of the above mentioned 23 languages.


U+0600–U+06FF Arabic
U+0750–U+077F Arabic Supplement
U+08A0–U+08FF Arabic Extended-A
U+0870–U+089F Arabic Extended-B
U+10EC0–U+10EFF Arabic Extended-C
U+FB50–U+FDFF Arabic Pres. Forms-A
U+FE70–U+FEFF Arabic Pres. Forms-B
U+1EE00–U+1EEFF Arabic Mathematical...
U+1EC70–U+1ECBF Indic Siyaq Numbers
U+1ED00–U+1ED4F Ottoman Siyaq Numbers
U+10E60–U+10E7F Rumi Numeral Symbols


SETU_TMP_DIR=/data/priyam/tmp/ spark-submit \
    --master "spark://SPK-DGX-O1:7077" \
    --driver-java-options "-Djava.io.tmpdir=$SETU_TMP_DIR" \
    --conf "spark.driver.extraJavaOptions=-Djava.io.tmpdir=$SETU_TMP_DIR" \
    --conf "spark.executor.extraJavaOptions=-Djava.io.tmpdir=$SETU_TMP_DIR" \
    --conf "spark.worker.dir=$SETU_TMP_DIR" \
    --conf "spark.local.dir=$SETU_TMP_DIR" \
    --conf "spark.sql.autoBroadcastJoinThreshold=1073741824" \
    --num-executors 96 \
    --executor-cores 1 \
    --executor-memory 4G \
    --driver-memory 32G \
    /data/priyam/setu/setu/helpers/redo_parquet.py \
    -g "/data/priyam/sangraha/parquets/sanskrit/*.parquet" \
    -s "/data/priyam/sangraha/parquets/sanskrit_new" \
    -p 50


python text-dedup/deduplicate-text-datasets/scripts/load_dataset \
        --path "arrow" \
        --name "sangraha-$lang" \
        --split "train" \
        --data_files "/data/priyam/sangraha/dedup/minhash/$lang/*.arrow" \
        --cache_dir "/data/priyam/cache" \
        --output "/data/priyam/sangraha/dedup/exact/$lang" \
        --column "text" \
        --google_repo_path "/data/priyam/setu/text-dedup/deduplicate-text-datasets"


SETU_DIR=/data/priyam/setu SETU_TMP_DIR=/data/priyam/tmp/ FILTER_DATA_ROOT=/data/priyam/setu/setu/data \
    spark-submit \
    --master spark://SPK-DGX-O2:7077 \
    --deploy-mode client \
    --driver-java-options -Djava.io.tmpdir=/data/priyam/tmp/ \
    --conf "spark.driver.extraJavaOptions=-Djava.io.tmpdir=/data/priyam/tmp/" \
    --conf "spark.executor.extraJavaOptions=-Djava.io.tmpdir=/data/priyam/tmp/" \
    --conf spark.worker.dir="/data/priyam/tmp/" \
    --conf spark.local.dir="/data/priyam/tmp/" \
    --num-executors 16 \
    --executor-cores 8 \
    --executor-memory 4G \
    --driver-memory 10G \
    --archives "/data/priyam/setu/dataproc/envs/setu.zip" \
    --conf 'spark.executorEnv.PYTHONPATH=setu.zip' \
    --conf 'spark.executorEnv.FILTER_DATA_ROOT=setu.zip/data' \
    run.py \
    --config /data/priyam/setu/configs/spark_dogri_config.json \
    --mode crawl \
    --run_local True \
    JSON2ParquetStage \
    --json_glob_path "/data/priyam/sangraha/pdf_texts/1/Assamese/*.json" \
    --language assamese \
    --j2p_samples_per_partition 1500 \
    --j2p_verbose False \
    --j2p_run_mode data \
    --j2p_parquet_output_path /data/priyam/sangraha/new_setu_test/assamese/1

spark-submit \
    --master spark://SPK-DGX-O2:7077 \
    --deploy-mode client \
    --driver-java-options -Djava.io.tmpdir=/data/priyam/tmp/ \
    --conf "spark.driver.extraJavaOptions=-Djava.io.tmpdir=/data/priyam/tmp/" \
    --conf "spark.executor.extraJavaOptions=-Djava.io.tmpdir=/data/priyam/tmp/" \
    --conf spark.worker.dir="/data/priyam/tmp/" \
    --conf spark.local.dir="/data/priyam/tmp/" \
    --num-executors 16 \
    --executor-cores 8 \
    --executor-memory 32G \
    --driver-memory 50G \
    clean_parquets.py
